The ability to make predictions based on online searches in various contexts is gaining substantial interest in both research and practice. This study investigates a novel application of correlated online searches in predicting stock performance across supply chain partners. If two firms are economically dependent through a supply chain relationship and if information related to both firms diffuses in the market slowly or rapidly, then our ability to predict stock returns increases or decreases, respectively. We use online cosearches of stock as a proxy for the extent of information diffusion across supply chain-related firms. We identify publicly traded supply chain partners using Bloomberg data and construct cosearch networks of supply chain partners based on the weekly coviewing pattern of these firms on Yahoo! Finance. Our analyses show that the cosearch intensity across supply chain partners helps determine cross-return predictability. When investors of a focal stock pay less attention to its supply chain partners, we can use lagged partner returns to predict the future return of the focal stock. When investors' coattention to focal and partner stocks is high, the predictability is low. Our simulated trading strategy using returns of supply chain partners with low coattention generates a significant and positive return above the market returns and performs better than the previously established trading strategy using returns of all supply chain partners. The online appendix is available at .  
Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing low-quality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.  
We study the impact of changes in the competitors' listings in organic search results on the performance of sponsored search advertisements. Using data from an online retailer's keyword advertising campaign, we measure the impact of organic competition on both click-through rate and conversion rate of sponsored search advertisements. We find that an increase in organic competition leads to a decrease in the click performance of sponsored advertisements. However, organic competition helps the conversion performance of sponsored ads and leads to higher revenue. We also find that organic competition has a higher negative effect on click performance than does sponsored competition. Our results inform advertisers on how the presence of organic results influences the performance of their sponsored advertisements. Specifically, we show that organic competition acts as a substitute for clicks, but has a complementary effect on the conversion performance.  
Scholars within fields frequently introspect about the evolution of their research disciplines, asking if the discipline is developing a cumulative research tradition, influencing scholarly work in other fields, and expanding its boundaries. In this spirit, I examine the intellectual structure and evolution of one disciplinary journal, ISR, over the most recent four-year period (2012-2015). I use a census of citation data and classify journals citing ISR and cited by ISR into the disciplines they represent. Analyzing 12,833 citations to ISR from 100 journals, and 7,731 citations from ISR to papers published in 67 journals I find that, consistent with prior studies, ISR reflects the multidisciplinarity of the information systems (IS) discipline, its boundaries are expanding to include new disciplines, and the journal draws extensively on other IS journals. I raise some questions for further introspection about the impact and visibility of IS research.  
We explore how "Red Queen" competition is increasing the competitive premium on "evolvable" information systems (IS). Ephemeral market advantage coupled with relentless innovation spawning trends such as the Internet-of-Things and additive manufacturing are amplifying the importance of evolvable systems across all industries. We discuss uncharted theoretical and empirical territory for IS research on evolvable systems. The elusiveness of some of these phenomena to other disciplines offers a unique opportunity for IS scholars.  
For organizations to achieve the benefits of new information technology (IT) systems, their users must adopt and then actually use these new systems. Recent models help to articulate the potentially different explanations for why some users will adopt and then continue using new technologies, but these models have not explicitly incorporated IT knowledge. This is particularly important in contexts where the user base may be non-IT professionals--i.e., the users may vary substantially in their basic IT knowledge. We draw on psychology to argue that in situations where there is a wide variance in actual IT knowledge, there will often exist a U-shaped relationship between actual and self-perceived IT knowledge such that the least knowledgeable believe themselves to be highly knowledgeable. We then draw on individual-level adoption theories to argue that users with high self-perceived IT knowledge will be more likely to adopt new technologies and do so faster. We also draw on individual-level continuance theories to argue that users with low actual IT knowledge will be more likely to discontinue using new technologies and do so faster. We test our expectations using a proprietary data set of 225 sales professionals in a large Indian pharmaceutical company that is testing a new customer relationship management system. We find strong support for our hypotheses.  
Increasingly, new forms of organizing for knowledge production are built around self-organizing coproduction community models with ambiguous role definitions. Current theories struggle to explain how high-quality knowledge is developed in these settings and how participants self-organize in the absence of role definitions, traditional organizational controls, or formal coordination mechanisms. In this article, we engage the puzzle by investigating the temporal dynamics underlying emergent roles on individual and organizational levels. Comprised of a multilevel large-scale empirical study of Wikipedia stretching over a decade, our study investigates emergent roles in terms of prototypical activity patterns that organically emerge from individuals' knowledge production actions. Employing a stratified sample of 1,000 Wikipedia articles, we tracked 200,000 distinct participants and 700,000 coproduction activities, and recorded each activity's type. We found that participants' role-taking behavior is turbulent across roles, with substantial flow in and out of coproduction work. Our findings at the organizational level, however, show that work is organized around a highly stable set of emergent roles, despite the absence of traditional stabilizing mechanisms such as predefined work procedures or role expectations. This dualism in emergent work is conceptualized as "turbulent stability." We attribute the stabilizing factor to the artifact-centric production process and present evidence to illustrate the mutual adjustment of role taking according to the artifact's needs and stage. We discuss the importance of the affordances of Wikipedia in enabling such tacit coordination. This study advances our theoretical understanding of the nature of emergent roles and self-organizing knowledge coproduction. We discuss the implications for custodians of online communities as well as for managers of firms engaging in self-organized knowledge collaboration.  
The effect of information technology (IT) on employment is a crucial question in today's economy given the increased digitization of work. To analyze the relationship between IT use and firm-level employment, we examine the longitudinal role of IT use in the firm's total number of employees. Our data set comes from the emerging economy of Turkey, and it represents firms of different sizes and industries. The data capture the firm's use of enterprise applications, such as enterprise resource planning and customer relationship management, and the use of Web applications, such as e-banking and e-government. Our empirical specifications exploit both within-firm and between-firm variations to show the positive effect of IT use on firm-level employment, which varies across IT applications over time. Interestingly, we find that the effects of the use of enterprise applications materialize after two years, whereas the effects of the use of Web applications are realized in the current year. We also examine whether the role of IT use in firm-level employment are moderated by firm size, average wage rate, and industry technology intensity. The long-term effects of the use of enterprise applications on firm-level employment are more pronounced in larger firms, with higher average wages, and in high-technology industries. The results are robust to alternative specifications and tests that address causality and endogeneity concerns. Implications for research, practice, and public policy are discussed.  
Hypercompetitive consumer software markets pit incumbents against free alternatives and pirates. Although the extant literature has studied firm level strategic responses to consumer heterogeneity and piracy, there is a lack of understanding of consumer reactions to digital goods choice sets that include firm product extensions such as the introduction of premium or free alternatives. With context-dependent preferences as the theoretical basis, this study systematically examines the impact of piracy controls and product line extensions on welfare in a consumer software market context (i.e., willingness to pay (WTP) and changes in consumer and producer surplus). In two controlled experiments using double-bound-dichotomous-choice WTP elicitation, we investigate how piracy controls and product line extensions impact two different platforms of the same software (PC Adobe applications and mobile Adobe applications) in terms of propensity to pirate and WTP. We show that introducing a premium or free vertical extension has different impacts on consumers' WTP for the focal product depending on whether it is a low-cost or high-cost market even when controlling for individual differences, such as price fairness perceptions, product feature value, brand perceptions, etc. By contrast, piracy controls reduce piracy rates but have a limited impact on consumer WTP for the focal product in both contexts. By calculating the overall welfare of the market, we show that there is alignment in consumer and producer interests at current and estimated optimal price levels in both high-cost and low-cost markets. However, the introduction of a free product extension leads to a higher surplus in the high-cost market, whereas the introduction of the premium product extension leads to a higher surplus in the low-cost market.  
There is significant information asymmetry in the information technology (IT) outsourcing market. Clients are uncertain about vendors' capabilities and vendors are uncertain about clients' requirements. Prior literature has examined many devices to reduce such information asymmetry, e.g., vendor reputation, client-vendor prior relationship, vendors' Capability Maturity Model (CMM) rating, vendor location, and technological diversity of the vendor. We examine the impact of (to our knowledge) a hitherto unconsidered device, i.e., the use of an advisor. In the context of global sourcing, third-party advisors, with their accumulated knowledge of client requirements and the vendor landscape, can mitigate the information asymmetry between clients and vendors. However, in an extensive data set of IT outsourcing contracts going back two decades we found use of advisors to be rare (less than 5% of contracts go through an advisor). This motivates us to rigorously analyze their impact on clients and vendors as an open empirical question. Using a data set of 753 large IT outsourcing contracts, and through a series of econometric specifications and robustness tests, we establish that the presence of an advisor is associated with higher revenue for vendors and more positive contract outcomes. This analysis presents what is to our knowledge the first concrete evidence that third-party advisors can mitigate the information asymmetry in the IT outsourcing market and lead to better matching that benefits clients as well as vendors.  
Mitigating preventable readmissions, where patients are readmitted for the same primary diagnosis within 30 days, poses a significant challenge to the delivery of high-quality healthcare. Toward this end, we develop a novel, predictive analytics model, termed as the beta geometric Erlang-2 (BG/EG) hurdle model, which predicts the propensity, frequency, and timing of readmissions of patients diagnosed with congestive heart failure (CHF). This unified model enables us to answer three key questions related to the use of predictive analytics methods for patient readmissions: whether a readmission will occur, how often readmissions will occur, and when a readmission will occur. We test our model using a unique data set that tracks patient demographic, clinical, and administrative data across 67 hospitals in North Texas over a four-year period. We show that our model provides superior predictive performance compared to extant models such as the logit, BG/NBD hurdle, and EG hurdle models. Our model also allows us to study the association between hospital usage of health information technologies (IT) and readmission risk. We find that health IT usage, patient demographics, visit characteristics, payer type, and hospital characteristics, are significantly associated with patient readmission risk. We also observe that implementation of cardiology information systems is associated with a reduction in the propensity and frequency of future readmissions, whereas administrative IT systems are correlated with a lower frequency of future readmissions. Our results indicate that patient profiles derived from our model can serve as building blocks for a predictive analytics system to identify CHF patients with high readmission risk.  
How is value created in an online community (OC) over time? We explored this question through a longitudinal field study of an OC in the healthcare arena. We found that multiple kinds of value were produced and changed over time as different participants engaged with the OC and its evolving technology in various ways. To explain our findings, we theorize OC value as performed through the ongoing sociomaterial configuring of strategies, digital platforms, and stakeholder engagement. We develop a process perspective to explain these dynamics and identify multiple different kinds of value being created by an OC over time: financial, epistemic, ethical, service, reputational, and platform. Our research points to the importance of expanding the notion of OC users to encompass a broader understanding of stakeholders. It further suggests that creating OC value increasingly requires going beyond a dyadic relationship between the OC and the firm to encompassing a more complex relationship involving a wider ecosystem of stakeholders.  
In many online communities, users reveal innovative and potentially valuable intellectual property (IP) under conditions that entail the risk of theft and imitation. When there is rivalry and formal IP law is not effective, this could lead to underinvestment or withholding of IP, unless user-organized norms compensate for these shortcomings. This study is the first to explore the characteristics and functioning of such a norms-based IP system in the setting of anonymous, large-scale, and loose-knit online communities. To do so, we use data on the Threadless crowdsourcing community obtained through netnography, a survey, and a field experiment. On this basis, we identify an integrated system of well-established norms that regulate the use of IP within this community. We analyze the system's characteristics and functioning, and we find that the "legal certainty" it provides is conducive to cooperation, cumulative effects, and innovation. We generalize our findings from the case by developing propositions aimed to spark further research. These propositions focus on similarities and differences between norms-based IP systems in online and off-line settings, and the conditions that determine the existence of norms-based IP systems as well as their form and effectiveness in online communities. In this way, we contribute to the literatures on norms-based IP systems and online communities and offer advice for the management of crowdsourcing communities.  
A consumer typically visits an online store a few times before making a purchase decision, and on each visit spends some time browsing the store. The durations of these visits vary not only across consumers but also for a given consumer across multiple visits. We argue that the amount of time that a consumer spends on the first visit to a website depends on how she is drawn to the website. We find that the duration of the first visit is influenced by the advertising tool-banner ad or search engine-used to attract consumers to the website. The durations of subsequent visits are influenced by the durations of earlier visits. The search durations are also influenced by the visit day of the week and time of day. In this paper, we develop a multiple-spell competing risk model to capture the underlying stochastic process, chief elements of which are two interrelated processes: a duration process and a transition process. The multistate, multiple-spell model allows us to identify a window of opportunity, within which the purchase probability is higher than the exit probability. Online salespersons should target site visitors during this window of opportunity. The model, which is calibrated on clickstream data obtained from a major online vendor, can also be used to determine the bid price strategy for search engine ads.  
Bidders in larger ascending combinatorial auctions face a substantial coordination problem, which has received little attention in the literature. The coordination problem manifests itself by the fact that losing bidders need to submit nonoverlapping package bids that are high enough to outbid the standing winners. We propose an auction format, which leverages the information that the auctioneer collects throughout the auction about the preferences of individual bidders and suggests prices for the members of losing bidder coalitions, which in total would make a given coalition winning. We model the bidder's bundle selection problem as a coordination game, which provides a theoretical rationale for bidders to agree to these prices, and highlights the role of the auctioneer in providing relevant information feedback. Results of extensive numerical simulations and experiments with human participants demonstrate that this type of pricing substantially reduces the number of auction rounds and bids necessary to find a competitive equilibrium, and at the same time significantly increases auction efficiency in the lab. This rapid convergence is crucial for the practical viability of combinatorial auctions in larger markets. The online appendix is available at .  
Information technology (IT) has created new patterns of digitally-mediated collaboration that allow open sourcing of ideas for new products and services. These novel sociotechnical arrangements afford finely-grained manipulation of how tasks can be represented and have changed the way organizations ideate. In this paper, we investigate differences in behavioral decision-making resulting from IT-based support of open idea evaluation. We report results from a randomized experiment of 120 participants comparing IT-based decision-making support using a rating scale (representing a judgment task) and a preference market (representing a choice task). We find that the rating scale-based task invokes significantly higher perceived ease of use than the preference market-based task and that perceived ease of use mediates the effect of the task representation treatment on the users' decision quality. Furthermore, we find that the understandability of ideas being evaluated, which we assess through the ideas' readability, and the perception of the task's variability moderate the strength of this mediation effect, which becomes stronger with increasing perceived task variability and decreasing understandability of the ideas. We contribute to the literature by explaining how perceptual differences of task representations for open idea evaluation affect the decision quality of users and translate into differences in mechanism accuracy. These results enhance our understanding of how crowdsourcing as a novel mode of value creation may effectively complement traditional work structures.  
The increasing adoption of product recommendation agents (PRAs) by e-commerce merchants makes it an important area of study for information systems researchers. PRAs are a type of Web personalization technology that provides individual consumers with product recommendations based on their product-related needs and preferences expressed explicitly or implicitly. Whereas extant research mainly assumes that such recommendation technologies are designed to benefit consumers and focuses on the positive impact of PRAs on consumers' decision quality and decision effort, this study represents an early effort to examine PRAs that are designed to produce their recommendations on the basis of benefiting e-commerce merchants (rather than benefiting consumers) and to investigate how the availability and the design of warning messages (a potential detection support mechanism) can enhance consumers' performance in detecting such biased PRAs. Drawing on signal detection theory, the literature on warning messages, and the literature on message framing, we identified two content design characteristics of warning messages—the inclusion of risk-handling advice and the framing of risk-handling advice—and investigated how they influence consumers' detection performance. The results of an online experiment reveal that a simple warning message without accompanying advice on how to detect bias is a double-edged sword, because it increases correct detection of biased PRAs (hits) at the cost of increased incorrect detection (false alarms). By contrast, including in warning messages risk-handling advice about how to check for bias (particularly when the advice is framed to emphasize the loss from not following the advice) increases correct detection and, more importantly, also decreases incorrect detection. The patterns of findings are in line with the predictions of signal detection theory. With an enriched understanding of how the availability and the content design of warning messages can assist consumers in the context of PRA-assisted online shopping, the results of this study serve as a basis for future theoretical development and yield valuable insights that can guide practice and the design of effective warning messages.  
Diversity has attracted much attention within the information systems (IS) field, with literature concentrating on diversity in topics and methods. These constitute two of three identified areas of research field diversity; the little-investigated third area includes demographic and social diversity of researchers. This study explores this gap for researchers comprising the editorial advisory boards (EABs) of 52 IS journals and links the underexplored types of diversity to topic diversity. The journals are categorized into seven intellectual communities, using topic affinity of journal content, and a social network of EAB members constructed from board interlocks. The network structure appears to reflect the topic-based community links. Journal communities are aggregated into two components of the social network: a business-school-related core set of journals and a more diverse computing- and engineering-related periphery. The strong ties at the network center do not necessarily reflect journal status. The observed combination of focus and diversity is consistent with a polycentric view of the IS field. Findings suggest low demographic diversity in the field and that demographic diversity correlates with other types of diversity. The field's separation into business core and computing periphery is highlighted as potentially challenging to the IS field's identity.  
Individuals' actions in online social contexts are growing increasingly visible and traceable. Many online platforms account for this by providing users with granular control over when and how their identity or actions are made visible to peers. However, little work has sought to understand the effect that a user's decision to conceal information might have on observing peers, who are likely to refer to that information when deciding on their own actions. We leverage a unique impression-level data set from one of the world's largest online crowdfunding platforms, where contributors are given the option to conceal their username or contribution amount from public display, with each transaction. We demonstrate that when campaign contributors elect to conceal information, it has a negative influence on subsequent visitors' likelihood of conversion, as well as on their average contributions, conditional on conversion. Moreover, we argue that social norms are an important driver of information concealment, providing evidence of peer influence in the decision to conceal. We discuss the implications of our results for the provision of online information hiding mechanisms, as well as the design of crowdfunding platforms and electronic markets more generally.  
We examine the role of granular privacy controls on dynamic content-sharing activities and disclosure patterns of Facebook users based on the exogenous policy change in December 2009. Using a unique panel data set, we first conduct regression discontinuity analyses to verify a discontinuous jump in context generation activities and disclosure patterns around the time of the policy change. We next estimate unobserved effects models to assess the short-run and long-run effects of the change. Results show that Facebook users, on average, increase use of wall posts and decrease use of private messages after the introduction of granular privacy controls. Also, users' disclosure patterns change to reflect the increased openness in content sharing. These effects are realized immediately and over time. More importantly, we show that user-specific factors play crucial roles in shaping users' varying reactions to the policy change. While more privacy sensitive users (those who do not reveal their gender and/or those who have exclusive disclosure patterns ex ante) share more content openly and less content secretly than before, less privacy sensitive users (those who reveal their gender and/or those who have inclusive disclosure patterns ex ante) share less content openly and more content secretly after the change. Hence, the policy change effectively diminishes variation among Facebook users in terms of content-generation activities and disclosure patterns. Therefore, characterizing the privacy change as a way to foster openness across all user categories does not reveal the change's true influence. Although an average Facebook user seems to favor increased openness, the policy change has different impacts on various groups of users based on their sensitivity to privacy, and this impact is not necessarily toward increased openness. To our knowledge, this is the first study that relies on observational data to assess the impact of a major privacy change on dynamic content-sharing activities and the resulting disclosure patterns of Facebook users.  
Organizations are setting up online forums to obtain inputs and feedback from key stakeholders, such as employees, customers, and citizens. Examples of such virtual spaces are online policy deliberation forums (OPDFs) initiated by government organizations to garner citizens' views on policy issues. Incorporating the inputs from these forums can result in more inclusive policies for societal benefit. Yet, as with other such forums, a common issue facing OPDFs is the sustainability of participation. When examining this issue, previous research has mostly explored the participation antecedents of existing contributors. However, engaging lurkers is also important, because these forums need to compensate for contributor attrition and become more effective with greater reach. Thus motivated, this study develops a model to explain the antecedents of both contributors' and lurkers' participation deriving from public participation and information technology-enabled public goods theories. It hypothesizes differences in the antecedents for contributors versus lurkers based primarily on construal level theory. The model was empirically validated through a survey of contributors and lurkers in a nationwide OPDF. The results reveal significant differences in the participation antecedents of the two groups as hypothesized. Specifically, contributors are influenced by political career benefit and political efficacy motives, whereas lurkers' future participation intention is driven by collective benefits, possession of civic skills, and mobilization. Furthermore, perceived connectivity of the OPDF directly influences participation intention for contributors and indirectly impacts participation intention for both groups via perceived communality. Perceived communality, on the other hand, influences collective and persuasion benefits for both contributors and lurkers. These findings are useful for understanding and promoting participation through differential strategies for contributors and lurkers in OPDFs in particular, and by extension, other feedback or online forums.  
In some online labor markets, workers are paid by the task, choose what tasks to work on, and have little or no interaction with their (usually anonymous) buyer/employer. These markets look like true spot markets for tasks rather than markets for employment. Despite appearances, we find via a field experiment that workers act more like parties to an employment contract: workers quickly form wage reference points and react negatively to proposed wage cuts by quitting. However, they can be mollified with 'reasonable' justifications for why wages are being cut, highlighting the importance of fairness considerations in their decision making. We find some evidence that 'unreasonable' justifications for wage cuts reduce subsequent work quality. We also find that not explicitly presenting the worker with a decision about continuing to work eliminates 'quits,' with no apparent reduction in work quality. One interpretation for this finding is that workers have a strong expectation that they are party to a quasi-employment relationship where terms are not changed, and the default behavior is to continue working.  
Motivated by the pervasive discrepancy among the pricing schemes of data services, this paper investigates the selection of pricing metrics (variables) and the corresponding pricing plans. We construct a stylized model in which a monopoly data services seller faces heterogeneous consumers whose utilities depend on the usage and the connection speed. We examine three options for the seller to conduct the second-degree (indirect) price discrimination: by minutes, by gigabytes (Gigs), and by megabytes per second (Mbps). We show that the after-sales self-selection behaviors have a significant impact on the seller's profitability, and it leads to a first-order influence on the pricing metric selection. We prove that either pricing by Gigs or Mbps can be optimal. Pricing by Gigs can dominate pricing by Mbps even if the consumer's utility is more sensitive in changes in the connection speed. We also find that when incorporating the bandwidth costs or congestion costs, pricing by Mbps becomes more attractive as it allows the seller to directly control the congestion effect. These findings may help practitioners to develop their own pricing plans and pricing metrics selection.  
This study examines cloud computing spot pricing dynamics and the influence of latency on those pricing dynamics. Using the Amazon Elastic Compute Cloud U.S. East and West market spot instance pricing and latency intraday data from April 9, 2010, to May 22, 2011, we find considerable time variation in spot instance prices, and prices are often persistently higher in the West. Bivariate vector autoregressive model results show that within-market autoregressive pricing effects are larger than across-market effects. We also document that over 70% of the relative price discovery occurs in the East market. Our regression results further show that East-West latency differentials have a significantly positive effect on East-West pricing differentials. Latency creates a dynamic pricing wedge that widens or narrows conditional on the latency differentials. Using an error correction model, the speed of adjustment from long-run pricing convergence errors causes the short-run price differential to narrow, but the adjustment does not completely offset the price differential.  
Internet service providers (ISPs) are experimenting with a business model that allows content providers (CPs) to subsidize Internet access for end consumers. In this study, we develop a game-theoretical model to analyze the effects of this sponsorship of consumer data usage. We find that the ISP's optimal network management choice of data sponsorship crucially depends on market conditions, such as the revenue rates of CPs and the fit cost of consumers. If the fit cost is low, the ISP will either allow both CPs to subsidize consumers' Internet access, or will allow only the more competitive CP to subsidize, depending on the per-consumer revenue generation rates of CPs. If the fit cost is high, it is in the ISPs interest not to allow any subsidization. We also identify conditions under which the ISP's network management choices of data sponsorship deviate from social optimum. These results should be of interest to the telecom industry as it searches additional revenue models, and to online CPs competing for customer loyalty. It should also be of interest to policymakers investigating into this issue.  
Online social networks greatly facilitate social exchange among friends. At times, for amusement, individuals may be targeted by friends' playful teases, which often involve exposing individuals' private embarrassing information, such as information that reveals their past indecent behavior, mischief, or clumsiness. Although individuals sometimes do enjoy the humor, they might also be offended by the involuntary exposure. Drawing on social exchange theory, this paper elucidates the consequences of an embarrassing exposure in online social networks. Specifically, this study examines the effects of information dissemination and network commonality on individuals' exchange assessment as well as how this assessment shapes their behavioral responses. The results of our experiment provide strong evidence that information dissemination and network commonality jointly influence individuals' perceived privacy invasion and perceived relationship bonding. In addition, whereas perceived privacy invasion increases transactional avoidance, it reduces approach behavior. Furthermore, whereas perceived relationship bonding impedes both transactional avoidance and interpersonal avoidance, it leads to approach behavior. The theoretical and practical implications of the findings are discussed.  
This paper extends prior research on the software vendors' optimal release time and patching strategy in the context of cloud computing and software as a service (SaaS). Traditionally, users are responsible for running on-premises software; by contrast, a vendor is responsible for running SaaS software, and the SaaS vendor incurs a larger proportion of defect-related costs than a vendor of on-premises software. We examine the effect of this difference on a vendor's choice of when to release software and the proportion of software defects to fix. Surprisingly, we find that, despite incurring a larger proportion of defect-related costs, it is optimal for the SaaS vendor to release software earlier and with more defects, and to patch a smaller proportion of defects, than the on-premises software vendor. Even though the SaaS vendor incurs higher defect-related costs, he obtains a larger profit than the traditional vendor. In addition, we find that for a vendor who uses the SaaS model, the optimal number of defects after patching may be lower than the socially efficient outcome. This occurs despite the fact that the number of defects after patching in the SaaS model is higher than in the traditional on-premises model.  
Mandatory security standards that force firms to establish minimum levels of security controls are enforced in many domains, including information security. The information security domain is characterized by multiple intertwined security controls, not all of which can be regulated by standards, but compliance with existing security standards is often used by firms to deflect liability if a security breach occurs. We analyze a stylized setting where a firm has two security controls that are linked in either a serial or a parallel configuration. One control is directly regulated by a security standard, whereas the other one is not. We show that a higher security standard does not necessarily lead to a higher firm security. Furthermore, the conditions under which a higher standard hurts the firm security are sharply different in the two-serial and parallel-configurations. If standard compliance leads to reduced liability for a firm following a breach, such liability reduction in turn weakens the tie between the standard and firm security. Under a setting in which the firm meets the optimal standard set by a policy maker, both firm security and social welfare are higher when the damage to the firm following a breach takes a higher share of the total damage to social welfare, and also when the firm takes a larger share of liability.  
A core activity in information systems development involves building a conceptual model of the domain that an information system is intended to support. Such models are created using a conceptual-modeling (CM) grammar. Just as high-quality conceptual models facilitate high-quality systems development, high-quality CM grammars facilitate high-quality CM. This paper provides a new perspective on ways to improve the quality of the semantics of CM grammars. For many years, the leading approach to this topic has relied on ontological theory. We show, however, that the ontological approach captures only half the story; it needs to be coupled with a logical approach. We explain how the ontological and logical qualities of CM grammars interrelate. Furthermore, we outline three contributions of a logical approach to evaluating the quality of CM grammars: a means of seeing some familiar CM problems in simpler ways, illumination of new problems, and proving the benefit of modifying existing CM grammars in particular ways. We demonstrate these benefits in the context of the Entity-Relationship grammar. More generally, our paper opens a new area of research with many opportunities for future research and practice.  
In this paper, we examine the challenges around the development and scalability of information infrastructures. We identify two possible solutions proposed in the literature, one emphasizing more top-down control and the need for a clear IT governance framework, and a second arguing for a more flexible approach since absolute control is impossible and only leads to drift and unintended outcomes. We suggest that there is a clear gap in the literature in better understanding how to govern the development of information infrastructures using a bottom-up approach. We build on research that approaches IS development as a collective action problem and focus on how different actors frame the infrastructure as a public and private good, and how the framing process is underpinned by actors' different ideologies. We use our theoretical approach to examine the framing of the development of a regional health information infrastructure in Crete. Our analysis examines how different actors frame the infrastructure as a collective action good and explore their ideological positioning. We explore the struggle around meanings attributed to the good over time as being a public or private one in establishing or sustaining relations of power, and how legitimacy is challenged or reinforced. Finally, we develop contributions on the collective action challenges in infrastructure development and suggest how a polycentric approach to governance might be further developed to promote the ongoing cultivation of information infrastructures from the bottom up.  
Taking advantage of the opportunities created by the price adjusted performance improvement in information technology (IT) depends in part on the ability of IT capital to substitute for other inputs in production. Studies in the information systems literature and most economics training examining the substitution of IT capital for other inputs use the Allen elasticity of substitution (AES). We present a less-well-known measure for the elasticity of substitution, the Morishima elasticity of substitution (MES). In contrast to the AES, which is misleading when there are three or more inputs--such as non-IT capital, labor, and IT capital--the MES provides a substitution measure where the scale is meaningful, and the measure differs depending on which price is changing. This is particularly important for IT capital, because prices have been declining, and there is evidence that IT capital can substitute for non-IT capital or labor in a qualitatively different way than non-IT capital and labor substitute for each other. Methodologically, we also show the impact of imposing local regularity--for example, monotonicity of output from increases in inputs--which we do through Bayesian methods employed to estimate the underlying functions that are used to calculate various measures of substitution. We demonstrate the importance of the MES as an underrecognized measure of substitution and the impact of imposing local regularity using an economy-wide industry-level data set covering 1998-2009 at the three-digit North American Industry Classification System code level. Our MES results show that reductions in the price of IT capital increase the quantity of IT capital in use, but are unlikely to change the input share of IT capital--the value of IT capital as a proportion of the value of all inputs--in contrast to major studies using the AES. In addition, estimates for both elasticities of substitution are more stable after imposing local regularity. Both of these advances--that is, the MES and imposing local regularity--have the potential to impact future work on IT productivity, IT pricing, IT cost estimation, and any type of analysis that posits the substitution of IT capital for non-IT capital or labor.  
Based on our interactions with the key personnel of three different healthcare information exchange (HIE) providers in Texas, we develop models to study the sustainability of HIEs and participation levels in these networks. We first examine how heterogeneity among healthcare practitioners (HPs) (in terms of their expected benefit from the HIE membership) affects participation of HPs in HIEs. We find that, under certain conditions, low-gain HPs choose not to join HIEs. Hence, we explore several measures that can encourage more participation in these networks and find that it might be beneficial to (i) establish a second HIE in the region, (ii) propose more value to the low-gain HPs, or (iii) offer or incentivize value-added services. We present several other interesting and useful results that are somewhat counterintuitive. For example, increasing the highest benefit the HPs can get from the HIE might decrease the number of HPs that want to join the HIE. Furthermore, since the amount of funds from the government and the other agencies often changes (and will eventually cease), we analyze how the changes in the benefit HPs obtain from the HIE affect (i) participation in the network, (ii) the HIE subscription fee and the fee for value-added service, (iii) the number of HPs that request value-added service, and (iv) the net values of the HIE provider and HPs. We also provide guidelines for policy makers and HIE providers that may help them improve the sustainability of HIEs and increase the participation levels in these networks.  
We study the relationship between a client and a vendor in value co-creation environments such as information technology projects. We consider that the client gets utility from the project throughout the development period and that the effort levels are not verifiable if not monitored. The output is contingent on the effort levels of each party, and we allow these effort levels to be dynamic. Hence, the client needs to optimally decide the terms of payment structures so as to maximize her net value. We analyze the performance of different payment structures and find the best one for the client in diverse settings. We show that the remaining time of the project and the client's valuation of the project regulate the behavior of the effort levels and some other characteristics in the collaboration. We derive the conditions under which the client chooses not to monitor the vendor's effort and operates in a double moral hazard environment. In addition, we find that the equilibrium effort levels or the values the parties gain from the collaboration do not necessarily increase when the output becomes more sensitive to either party's effort. Based on the results of our model, we present several other managerial insights.  
We study social influence in an online music community. In this community, users can listen to and 'favorite' (or like) songs and follow the favoriting behavior of their social network friends-and the community as a whole. From an individual user's perspective, two types of information on peer consumption are salient for each song: total number of favorites by the community as a whole and favoriting by their social network friends. Correspondingly, we study two types of social influence: popularity influence, driven by the total number of favorites from the community as a whole, and proximity influence, due to the favoriting behavior of immediate social network friends. Our quasi-experimental research design applies a variety of empirical methods to highly granular data from an online music community. Our analysis finds robust evidence of both popularity and proximity influence. Furthermore, popularity influence is more important for narrow-appeal music compared to broad-appeal music. Finally, the two types of influence are substitutes for one another, and proximity influence, when available, dominates the effect of popularity influence. We discuss implications for design and marketing strategies for online communities, such as the one studied in this paper.  
Recently, several researchers provided overarching macromodels to explain individuals' privacy-related decision making. These macromodels—and almost all of the published privacy-related information systems (IS) studies to date—rely on a covert assumption: responses to external stimuli result in deliberate analyses, which lead to fully informed privacy-related attitudes and behaviors. The most expansive of these macromodels, labeled "Antecedents–Privacy Concerns-Outcomes" (APCO), reflects this assumption. However, an emerging stream of IS research demonstrates the importance of considering principles from behavioral economics (such as biases and bounded rationality) and psychology (such as the elaboration likelihood model) that also affect privacy decisions. We propose an enhanced APCO model and a set of related propositions that consider both deliberative (high-effort) cognitive responses (the only responses considered in the original APCO model) and low-effort cognitive responses inspired by frameworks and theories in behavioral economics and psychology. These propositions offer explanations of many behaviors that complement those offered by extant IS privacy macromodels and the information privacy literature stream. We discuss the implications for research that follow from this expansion of the existing macromodels.  
Firms have made extensive use of interorganizational systems (IOSs) to share knowledge and pursue superior joint performance. Contemporary firms are using IOSs to collaborate widely across the value chain and in an ever-expanding geographic landscape. Thus, institutional distance, which is the difference between the firms' respective institutional fields, has become a prominent challenge. In this study, we investigate the extent to which institutional distance affects IOS-enabled knowledge sharing and its impact on the joint performance of collaborating firms. We also explore the extent to which IOS adaptability could be a design solution for improving IOS-enabled knowledge sharing, given the challenge of institutional distance. Drawing on institutional theory, we propose that institutional distance, differentially influential via its normative, cognitive, and regulative aspects, not only reduces IOS-enabled knowledge sharing but also weakens the positive impact of such sharing on joint firm performance. Next, extending boundary object theory to the institutional context, we propose that IOS adaptability could be a solution to the challenge of institutional distance because it can directly strengthen IOS-enabled knowledge sharing as well as mitigate the negative effect of institutional distance on such sharing. Our hypotheses were tested through a field study that collected dyadic data from 141 distinct buyer/supplier channel relationships in 4 industries. The results from partial least squares modeling fully support our hypotheses with regard to cognitive distance, partially support those related to normative distance, but do not support those related to regulative distance. We discuss the implications of these findings for theory development and professional practice. The online appendix is available at .  
Online communities frequently create significant economic and relational value for community participants and beyond. It is widely accepted that the underlying source of such value is the collective flow of knowledge among community participants. We distinguish the conditions for flows of tacit and explicit knowledge in online communities and advance an unconventional theoretical conjecture: Online communities give rise to tacit knowledge flows between participants. The crucial condition for these flows is not the advent of novel, digital technology as often portrayed in the literature, but instead the technology's domestication by humanity and the sociality it affords. This conjecture holds profound implications for theory and research in the study of management and organization, as well as their relation to information technology.  
This paper examines how an organization's culture, and in particular its stance toward the pursuit of knowledge and innovation, matters when confronting new digitally enabled practices for generating novel insights. We draw on an in-depth interpretive study of how two innovation consulting firms encountered crowdsourcing for innovation. Our findings suggest that, although both organizations relied on a similar set of organizational arrangements in their daily consulting work, they enacted different positions vis-à-vis crowdsourcing: one firm further experimented with it, whereas the other rejected it altogether. These different positions emerged as organizational actors examined, framed, and evaluated crowdsourcing as an alternative for generating knowledge. To interpret these findings, we draw on philosophy of science and develop the concept of organizational epistemic stance, defined as an attitude that organizational actors collectively enact in pursuing knowledge. Our analysis suggests that when organizational actors encounter and explore information technology-enabled practices, such as crowdsourcing and big data analytics, they are likely to remain committed to their epistemic stance to frame such practices and judge their potential value for pursuing knowledge. This paper contributes to our understanding of encounters with, and adoption and diffusion of, new organizational practices and offers new ways of thinking about crowdsourcing.  
Although our general knowledge about open source communities is extensive, we are only beginning to understand the increasingly common practices by which corporations design software through engagement with these communities. In response, we combine design theorizing with field-study research (1) to analyze rich qualitative data from over 40 corporations participating in the Linux open source community and (2) to synthesize the observed corporate-open source community engagements into a new type of information systems design theory that we call responsive design. Empirically, we document how corporate participants in these contexts respond to market decisions, interdependent ideologies, and distributed relationships by continuously establishing and maintaining connections with community members; connections that stem from the social and material rules inherent in the open source community. Based on these observations, we create the theory of responsive design as a particular form of corporate software design which, beyond the inclusion of external participants, distinguishes itself from traditional monocentric design in which one corporation controls a dedicated team of software designers focused on solving an isolated and singular organizational problem. Guided by the principles of interconnection, opportunism, and domestication, we define responsive design as the kind of design approach that enables corporate participants to create and maintain productive design practices in response to the complex and dynamic landscapes of activities that are the foundation of corporate-communal engagements. We conclude with a discussion of the theoretical and practical implications of this new form of corporate software design.  
Business analytics has evolved from being a novelty used by a select few to an accepted facet of conducting business. Recommender systems form a critical component of the business analytics toolkit and, by enabling firms to effectively target customers with products and services, are helping alter the e-commerce landscape. A variety of methods exist for providing recommendations, with collaborative filtering, matrix factorization, and association-rule-based methods being the most common. In this paper, we propose a method to improve the quality of recommendations made using association rules. This is accomplished by combining rules when possible and stands apart from existing rule-combination methods in that it is strongly grounded in probability theory. Combining rules requires the identification of the best combination of rules from the many combinations that might exist, and we use a maximum-likelihood framework to compare alternative combinations. Because it is impractical to apply the maximum likelihood framework directly in real time, we show that this problem can equivalently be represented as a set partitioning problem by translating it into an information theoretic context--the best solution corresponds to the set of rules that leads to the highest sum of mutual information associated with the rules. Through a variety of experiments that evaluate the quality of recommendations made using the proposed approach, we show that (i) a greedy heuristic used to solve the maximum likelihood estimation problem is very effective, providing results comparable to those from using the optimal set partitioning solution; (ii) the recommendations made by our approach are more accurate than those made by a variety of state-of-the-art benchmarks, including collaborative filtering and matrix factorization; and (iii) the recommendations can be made in a fraction of a second on a desktop computer, making it practical to use in real-world applications.  
To motivate user contributions, user-generated content sites routinely deploy incentive hierarchies, where users achieve increasingly higher statuses in the community after achieving increasingly more difficult goals. Yet the existing empirical literature remains largely unclear whether such hierarchies are indeed effective in inducing user contributions. We gather data from a large online crowd-based knowledge exchange to answer this question, and draw on goal setting and status hierarchy theories to study users' contributions before and after they reach consecutive ranks on a vertical incentive hierarchy. We find evidence that even though these glory-based incentives may motivate users to contribute more before the goals are reached, user contribution levels drop significantly after that. The positive effect on user contribution appears only temporary. Moreover, such impacts are increasingly smaller for higher ranks. Our results highlight some unintended and heretofore undocumented effects of incentive hierarchies, and have important implications for business models that rely on user contributions, such as knowledge exchange and crowdsourcing, as well as the broader phenomenon of gamification in other contexts.  
Logistics outsourcing has increased with the commercialization of the Internet, implying a reduction in the corresponding transaction costs. The Internet-with its universal connectivity and open standards-radically enhanced information technology (IT) capabilities, and we hypothesize this has reduced external transaction costs relatively more than internal governance costs. Using transaction cost theory as a lens, we examine whether the commercialization of the Internet coincided with a move to the market in logistics-one of the most connected industries in the economy. We estimate the relationship between IT and outsourced logistics in a production function based on two data sets from 1987 to 2008. We find that the effects of IT on outsourced logistics have changed in the post-Internet era. After the commercialization of the Internet, an industry's own IT investment and outsourced logistics became complements, whereas they were not before. It suggests that because of the unique characteristics of the Internet as an enabler, IT reduced external transaction costs relatively more than internal governance costs. Consequently, industries favored the market form of the provision of logistics. We also find similar impacts of customers' IT investments on a focal industry's outsourced logistics. Previous studies argued that IT led to the shift from hierarchies to markets, or provided indirect evidence through measures of firm size or integration. Using a production theory model, our study provides systematic empirical evidence to support that the Internet enabled a move to the market in the provision of logistics.  
The diffusion of the Internet and digital technologies has enabled many organizations to use the open-content production model to produce and disseminate knowledge. While several prior studies have shown that the open-content production model can lead to high-quality output in the context of uncontroversial and verifiable information, it is unclear whether this production model will produce any desirable outcome when information is controversial, subjective, and unverifiable. We examine whether the open-content production model helps achieve a neutral point of view (NPOV) using data from Wikipedia's articles on U.S. politics. Our null hypothesis builds on Linus' Law, which argues that with enough eyeballs, all bugs are shallow. Our findings are consistent with a narrow interpretation of Linus' Law, namely, a greater number of contributors to an article makes an article more neutral. No evidence supports a broad interpretation of Linus' Law. Moreover, several empirical facts suggest the law does not shape many articles. The majority of articles receive little attention, and most articles change only mildly from their initial slant. Our study provides the first empirical evidence on the limit of Linus' Law. While many organizations believe that they could improve their knowledge production by leveraging communities, we show that in the case of Wikipedia, there are aspects, such as NPOV, that the community does not always achieve successfully.  
In this paper, we study the impact of increases in media coverage from two sources, newspapers and blogs, on firm founding rates in the context of technology-based entrepreneurship. Although increasing work in information systems (IS) has begun to investigate the effect of user-generated content on entrepreneurial behavior,limited attention has been devoted to how media affects firm founding or the boundary conditions of such an effect. Arguing for the direct effect of increased discourse in traditional and user-generated media in the information technology (IT) industry, results suggest that discourse in traditional media and blogs strongly influences IT firm founding rates. We further consider the differential impacts of media discourse on firm founding in different IT subsectors, over time, and in different locations. We test our hypotheses using entrepreneurial firm founding data from VentureXpert from 1998 to 2007, social media data from the three largest blogging platforms, and traditional media coverage from 11 major U.S. newspapers. Our work contributes to a better understanding of the concurrent effects of multiple forms of media on decision making and adds to the small but emerging literature addressing entrepreneurship-related research questions in IS.  
Though information technology (IT) transformation programs are gaining in importance, we know little about the nature of the challenges involved in such programs and how to manage them. Using grounded theory methodology, we conducted a multiyear case study of a large IT transformation program in a major commercial bank, during which we encountered the interrelated themes of paradoxes and ambidexterity. Grounded in our case, we construct a substantive theory of ambidexterity in IT transformation programs that identifies and explains the paradoxes that managers need to resolve in IT transformation programs. The ambidexterity areas we identified are (1) IT portfolio decisions (i.e., IT efficiency versus IT innovation), (2) IT platform design (i.e., IT standardization versus IT differentiation), (3) IT architecture change (i.e., IT integration versus IT replacement), (4) IT program planning (i.e., IT program agility versus IT project stability), (5) IT program governance (i.e., IT program control versus IT project autonomy), and (6) IT program delivery (i.e., IT program coordination versus IT project isolation). What weaves these six areas together is the combined need for IT managers to employ ambidextrous resolution strategies to ensure short-term IT contributions and continuous progress of IT projects while simultaneously working toward IT transformation program success as a foundation for IT-enabled business transformation. However, in addition to this commonality, we find that the nature of paradoxical tensions differs across the six areas and requires slightly different management strategies for paradox resolution. Ambidexterity areas (1), (2), and (3) are associated with IT transformation strategizing and, in addition to balancing short- and long-term goals, require the mutual accommodation and blending of business and IT interests in the spirit of IT-business partnering to achieve IT-enabled business change and IT-based competitiveness. Ambidexterity areas (4), (5), and (6) are associated with IT program and project execution and, in addition to balancing short- and long-term requirements, require a recurrent and dynamic act of balancing "local" needs at the IT project level and "global" needs at the IT program level.  
The author describes his responsibilities as editor-in-chief of "Information Systems Research" (ISR) and his plans for the kind of research to be published by the journal.
With the emergence of social media and Web 2.0, broadcasting in the online environment has evolved into a new form of marketing due to the much broader reach enabled by information technology. This paper quantifies the effect of artists' broadcasting activities on a well-known social media site for music, MySpace, on music sales. We employ a panel vector autoregression model to investigate the interrelationship between broadcasting promotions in social media and music sales, while controlling for influential factors such as advertising in traditional media channels, album prices, new music releases, user-generated content, and artist popularity. We characterize two types of broadcast messages in the MySpace context, personal and automated. We find that broadcasting in social media has a significant effect on sales even after controlling for the aforementioned factors, and more important, the effect mainly comes from personal messages rather than automated messages. We also show that the timing and content of personal messages play a role in affecting sales. Our findings point to the importance of conducting captivating conversations with customers in social media marketing.  
We investigate the impact of the intergenerational nature of services, via backward compatibility, on the adoption of multigenerational platforms. We consider a mobile Internet platform that has evolved over several generations and for which users download complementary services from third-party providers. These services are often intergenerational: newer platform generations are backward compatible with respect to services released under earlier generation platforms. In this paper, we propose a model to identify the main drivers of consumers' choice of platform generation, accounting for (i) the migration from older to newer platform generations, (ii) the indirect network effect on platform adoption due to same-generation services, and (iii) the effect on platform adoption due to the consumption of intergenerational services via backward compatibility. Using data on mobile Internet platform adoption and services consumption for the time period of 2001-2007 from a major wireless carrier in an Asian country, we estimate the three effects noted above. We show that both the migration from older to newer platform generations and the indirect network effects are significant. The surprising finding is that intergenerational services that connect subsequent generations of platforms essentially engender backward compatibility with two opposing effects. Whereas an intergenerational service may accelerate the migration to the subsequent platform generations, it may also, perhaps unintentionally, provide a fresh lease on life for earlier generation platforms due to the continued use of earlier generation services on newer platform generations.  
There are many contexts in which an 'everybody else is doing it' attitude is relevant. We evaluate the impact of this attitude in a multithreshold public goods game. We use a lab experiment to study the role of providing information about contribution behavior to targeted subsets of individuals, and its effect on coordination. Treatments include one in which no information is provided and three other treatments, i.e., where information is provided to a random sample of subjects; to those whose contributions are below the average of their group, and to those whose contributions are above the average of their group. We find that the random provision of information is no different than not providing information. More important, average contributions improve with targeted treatments. Coordination waste is also lower with targeted treatments. The insights from this research are more broadly relevant in the contexts of piracy, open innovation, and crowdfunding. The online appendix is available at  
Conspicuous consumption affects anyone who cares about social status; it has intrigued sociologists and economists for more than 100 years. The idea that conspicuous consumption can increase social status, as a form of social capital, has been broadly accepted, yet researchers have not been able to test this effect empirically. In this work, we provide empirical evidence by analyzing the digital footprints of purchases and social interactions in different virtual worlds. We use a multimethod approach, such that we both analyze transactional data and conduct a randomized field experiment. Virtual worlds, as artificial laboratories, offer the opportunity to analyze the social capital of their inhabitants, subsequent to their purchase of virtual prestige goods, which provides a means to empirically test hypotheses that would be nearly impossible to test in real-world settings. Our results are consistent with the notion that conspicuous consumption represents an investment in social capital.  
Through reimbursing a portion of the transactional amount to some consumers in a form of cash back, merchants are able to exercise third-degree price discrimination by offering two asymmetric prices via an online dual channel. To better understand such a novel pricing mechanism, we develop a game theoretical model and start our analyses with a market consisting of one merchant, one affiliate site, and consumers heterogeneous in their product valuation. From a price point of view, cash-back shopping appears to provide site users with a saving opportunity since the effective post-cash-back price they pay is perceived to be lower than the regular price targeted at nonusers. However, we find that under some conditions, this seemingly lower price could be actually higher, compared with the optimal uniform price when the merchant does not price discriminate. An important implication is that all consumers may end up suffering from higher prices in the presence of the cash-back mechanism. This surprising result, referred to as the cash-back paradox, defies a common intuition that a price-discriminating firm must raise the price for one segment of consumers but decrease it for the other. We also develop two extensions to seek explanations behind various industry practices. We find that it is in a merchant's best interest to affiliate with multiple sites, and the resulting competition improves overall market efficiency. Moreover, merchants who are disadvantageous in brand valuation should target price-sensitive consumers by strategically offering cash-back deals. Our results, consistent with several real-world observations, have useful implications for marketers. The online appendix is available at  
Drawing on the rational addiction framework, this study explores the digital vulnerabilities driven by dependence on mobile social apps (e.g., social network sites and social games). Rational addicts anticipate the future consequences of their current behaviors and attempt to maximize utility from their intertemporal consumption choices. Conversely, myopic addicts tend toward immediate gratification and fail to fully recognize the future consequences of their current consumption. In lieu of conducting self-report surveys or aggregatelevel demand estimation, this research examines addictive behaviors on the basis of consumption quantity at an individual level. To empirically validate rational addiction in the context of social app consumption, we collect and analyze 13-month, individual-level panel data on the weekly app usage of thousands of smartphone users. Results indicate that the average social app user conducts herself in a forward-looking manner and rationally adjusts consumption over time to derive optimal utility. The subgroup analysis, however, indicates that substantial variations in addictiveness and forward-looking propensities exist across demographically diverse groups. For example, addictive behaviors toward social network sites are more myopic in nature among older, less-educated, high-income groups. Additionally, the type of social app moderates the effects of demographic characteristics on the nature of addictive behaviors. We provide implications that policymakers can use to effectively manage mobile addiction problems, with the recommendations focusing on asymmetric social policies (e.g., information- and capacity-enhancing measures).  
Since the advent of the AIS Grand Vision Project of ICT-enabled Bright Society (in short, Bright ICT), there has been significant excitement as well as confusion about the concept. To resolve ambiguities about the types of research that are consistent with this vision, the notions of Restorative Bright ICT Research and Enriching Bright ICT Research are defined. In addition, we propose three perspectives that can differentiate Bright ICT research from traditional research approaches to create a disruptive impact on society. To address societal problems that are often global in scope, Bright ICT research recommends taking a holistic design of future society encompassing technologies and policies as well as business models driven by visionary principles. This paradigm can be an extension of design science for the scope of a Macro Information Society.  
System-generated alerts are ubiquitous in personal computing and, with the proliferation of mobile devices, daily activity. While these interruptions provide timely information, research shows they come at a high cost in terms of increased stress and decreased productivity. This is due to dual-task interference (DTI), a cognitive limitation in which even simple tasks cannot be simultaneously performed without significant performance loss. Although previous research has examined how DTI impacts the performance of a primary task (the task that was interrupted), no research has examined the effect of DTI on the interrupting task. This is an important gap because in many contexts, failing to heed an alert--the interruption itself--can introduce critical vulnerabilities. Using security messages as our context, we address this gap by using functional magnetic resonance imaging (fMRI) to explore how (1) DTI occurs in the brain in response to interruptive alerts, (2) DTI influences message security disregard, and (3) the effects of DTI can be mitigated by finessing the timing of the interruption. We show that neural activation is substantially reduced under a condition of high DTI, and the degree of reduction in turn significantly predicts security message disregard. Interestingly, we show that when a message immediately follows a primary task, neural activity in the medial temporal lobe is comparable to when attending to the message is the only task. Further, we apply these findings in an online behavioral experiment in the context of a web-browser warning. We demonstrate a practical way to mitigate the DTI effect by presenting the warning at low-DTI times, and show how mouse cursor tracking and psychometric measures can be used to validate low-DTI times in other contexts. Our findings suggest that although alerts are pervasive in personal computing, they should be bounded in their presentation. The timing of interruptions strongly influences the occurrence of DTI in the brain, which in turn substantially impacts alert disregard. This paper provides a theoretically grounded, cost-effective approach to reduce the effects of DTI for a wide variety of interruptive messages that are important but do not require immediate attention.  
We study operational and managerial problems arising in the context of security monitoring where sessions, rather than raw individual events, are monitored to prevent attacks. The objective of the monitoring problem is to maximize the benefit of monitoring minus the monitoring cost. The key trade-off in our model is that as more sessions are monitored, the attack costs should decrease. However, the monitoring cost would likely increase with the number of sessions being monitored. A key step in solving the problem is to derive the probability density of a system with n sessions being monitored with a session's age measured as the time elapsed since it last generated a suspicious event. We next optimize the number of sessions monitored by trading off the attack cost saved with the cost of monitoring. A profiling step is added prior to monitoring and a resulting two-dimensional optimization problem is studied. Through numerical simulation, we find that a simple size-based policy is quite robust for a very reasonable range of values and, under typical situations, performs almost as well as the two more sophisticated policies do. Also, we find that adopting a simplified policy without using the option of managing sessions using age threshold can greatly increase the ease of finding an optimal solution, and reduce operational overhead with little performance loss compared with a policy using such an option. The insights gained from the mechanics of profiling and monitoring are leveraged to suggest a socially optimal contract for outsourcing these activities in a reward-based contract. We also study penalty-based contracts. Such contracts (specifically, when the penalty is levied as a percentage of the monthly service fee) do not achieve the social optimum. We show how an appropriate penalty coefficient can be chosen to implement a socially optimal penalty-based contract. In addition, we provide a high-level comparison between reward- and penalty-based contracts. In a penalty-based contract, the setting of the fixed payment can be challenging because it requires additional knowledge of the total expected malicious event rate, which needs to be observed through a period of no monitoring.  
Information security (IS) threats are increasingly pervasive, and search engines are being used by the public as the primary tool for searching for relevant information. This research investigates the following two questions: (1) How can different IS threats be characterized and distinguished in terms of their risk characteristics? and (2) how are risk characteristics related to public searches for information on IS threats? Applying psychometric analysis, our analyses of survey data first show that unknown risk and dread risk are two underlying dimensions that can characterize different IS threats. Drawing broadly on the literature of information foraging theory, we examine the influence of risk characteristics on public searches for information on these threats. We utilize a search engine log to extract searches related to IS threats. We develop and estimate a system of equations with correlated individual-specific error terms using the Markov Chain Monte Carlo method. We find that the two risk characteristics exert differential impacts on information search behavior (including types of information sought, number of pages viewed, and length of query). The implications for IS research and practice are discussed.  
Compared to traditional organizations, online community leadership processes and how leaders emerge are not well studied. Previous studies of online leadership have often identified leaders as those who administer forums or have high network centrality scores. Although communication in online communities occurs almost exclusively through written words, little research has addressed how the comparative use of language shapes community dynamics. Using participant surveys to identify leading online community members, this study analyzes a year of communication network history and message content to assess whether language use differentiates leaders from other core community participants. We contribute a novel use of textual analysis to develop a model of language use to evaluate the utterances of all participants in the community. We find that beyond communication network position--in terms of formal role, centrality, membership in the core, and boundary spanning--those viewed as leaders by other participants, post a large number of positive, concise posts with simple language familiar to other participants. This research provides a model to study online language use and points to the emergent and shared nature of online community leadership.  
This study draws on distributive justice, human capital, and stigmatization theories to hypothesize relationships between relative pay gap and patterns of job mobility. Our study also expands the criterion space of job mobility by contrasting different job destinations when information technology (IT) professionals make job moves. We examine three job moves: (a) turnover to another IT job in a different firm, (b) turnaway-within to a non-IT job, and (c) turnaway-between to a different firm and a non-IT job. We analyze work histories spanning 28 years for 359 IT professionals drawn from the National Longitudinal Survey of Youth. We report three major findings. First, as hypothesized, larger relative pay gaps significantly increase the likelihood of job mobility. Second, IT males and IT females have different job mobility patterns. IT males are more likely to turn over than turn away-between when faced with a relative pay gap. Further, and contrary to predictions from human capital theory, IT males are more likely to turn away-within than turn over. This surprising finding suggests that the ubiquitous use of IT in other business functions may have increased the value of IT skills for non-IT jobs and reduced the friction of moving from IT to other non-IT positions. Third, and consistent with stigmatization arguments, IT females are more likely to turn away from IT than to turn over when faced with a relative pay gap. In fact, to reduce relative pay gaps, IT females tend to take on lower-status jobs that pay less than their IT jobs. We conclude this study with important theoretical, practical, and policy implications.  
Previous research into consumer choice of service channels studied the impact of online access as an addition to conventional service. Here, we study the impact of a compulsory migration to an online channel. We exploit a natural experiment in the implementation of a new federal government service to identify the causal effect of access channel on consumer choice. The government served western states through the Internet and telephone at all times. However, for the first 10 days, the government served the East through the Internet only. Comparing consumer responses in the East (only Internet service available) and West (both Internet and telephone service available), we find robust evidence that some consumers preferred telephone access. The unavailability of telephone service in the first 10 days resulted in a 4.3% loss of consumers who were otherwise interested in the service.  
The 15-year history of collaboration on Wikipedia offers insight into how peer production communities create knowledge. In this research, we combine disparate content and collaboration approaches through a social network analysis approach known as an affiliation network. It captures both how knowledge is transferred in a peer production network and also the underlying skills possessed by its contributors in a single methodological approach. We test this approach on the Wikipedia articles dedicated to medical information developed in a subcommunity known as a WikiProject. Overall, we find that the position of an article in the affiliation network is associated with the quality of the article. We further investigate information quality through additional qualitative and quantitative approaches including expert coders using medical students, crowdsourcing using Amazon Mechanical Turk, and visualization using network graphs. A review by fourth-year medical students indicates that the Wikipedia quality rating is a reliable measure of information quality. Amazon Mechanical Turk ratings, however, are a less reliable measure of information quality, reflecting observable content characteristics such as article length and the number of references.  
We study the effect of product market competition on the propensity to use corporate venture capital (CVC) as a part of an information technology (IT) firm's innovation strategy. Using novel measures of product market competition based on product descriptions from firm 10-K statements and accounting for potential endogeneity, we investigate how product market competition between 1997 and 2007 relates to the magnitude of CVC spending. We first find that firms in competitive markets make higher research and development (R&D) and CVC investments. In addition, we find that increasing product market competition leads to a shift away from internal R&D spending and into CVC. These movements are significantly stronger for technology leaders, i.e., firms with deep patent stocks, in the IT industry. We also find that CVC appears to be an effective way of exploiting external knowledge for technology leaders in the IT-producing industry, but not for technology slow starters. CVC investments lead to significantly more patent applications for technology leaders but no appreciable difference for slow starters. Our results provide new insights for theories of innovation in competitive, dynamic markets, potentially as part of a portfolio that includes internal R&D as well as open innovation models.  
Net neutrality (NN) is believed to prevent the emergence of exclusive online content, which yields Internet fragmentation. We examine the relationship between NN regulation and Internet fragmentation in a game-theoretic model that considers the interplay between termination fees, exclusivity, and competition between two Internet service providers (ISPs) and between two content providers (CPs). An exclusivity arrangement between an ISP and a CP reduces the CP's exposure to some end users, but it also reduces competition over ads among the CPs. Fragmentation arises in equilibrium when competition over ads among the CPs is very strong, the CPs' revenues from advertisements are very low, the content of the CPs is highly complementary, or the termination fees are high. We find that the absence of fragmentation is always beneficial for consumers, because they can enjoy all available content. Policy interventions that prevent fragmentation are thus good for consumers. However, results for total welfare are more mixed. A zero-price rule on traffic termination is neither a sufficient nor a necessary policy instrument to prevent fragmentation. In fact, regulatory interventions may be ineffective or even detrimental to welfare and are only warranted under special circumstances.  
Research findings on how participation in social networking sites (SNSs) affects users' subjective well-being are equivocal. Some studies suggest a positive impact of SNSs on users' life satisfaction and mood, whereas others report undesirable consequences such as depressive symptoms and anxiety. However, whereas the factors behind the positive effects have received significant scholarly attention, little is known about the mechanisms that underlie the unfavorable consequences. To fill this gap, this study uses social comparison theory and the responses of 1,193 college-age Facebook users to investigate the role of envy in the SNS context as a potential contributor to those undesirable outcomes. Arising in response to social information consumption, envy is shown to be associated with reduced cognitive and affective well-being as well as increased reactive self-enhancement. These preliminary findings contribute to the growing body of information systems research investigating the dysfunctional consequences of information technology adoption in general and social media participation in particular.  
This paper examines how information technology (IT) can contribute to value creation in horizontal acquisitions. We propose that acquisition value can be created when an acquirer redeploys its digital resources to its newly acquired businesses and consequently improves their operations. However, not all acquirers are equally capable of redeploying their digital resources. In this study, we propose two enabling factors pertaining to an acquirer's IT resource base: IT extensiveness and IT standardization. We argue that the magnitude of digital resource redeployment increases when the acquirer has had extensive use of IT systems within its existing business units and has standardized IT systems across its business units. Moreover, the relative strength of the IT resources of the acquirer, as compared to those of the acquired business, also affects the reuse of the acquirer's IT resources in its digital resource redeployment activities. We empirically test these hypotheses by tracking the IT and performance changes in 108 U.S. hospitals before and after they were acquired across a seven-year study timeframe.  
Trust in technology is an emerging research domain that examines trust in the technology artifact instead of trust in people. Although previous research finds that trust in technology can predict important outcomes, little research has examined the effect of unmet trust in technology expectations on trusting intentions. Furthermore, both trust and expectation disconfirmation theories suggest that trust disconfirmation effects may be more complex than the linear expectation disconfirmation model depicts. However, this complexity may only exist under certain contextual conditions. The current study contributes to this literature by introducing a nonlinear expectation disconfirmation theory model that extends understanding of trust-in-technology expectations and disconfirmation. Not only does the model include both technology trust expectations and technology trusting intention, it also introduces the concept of expectation maturity as a contextual factor. We collected data from three technology usage contexts that differ in expectation maturity, which we operationalize as length of the introductory period. We find that the situation, in terms of expectation maturity, consistently matters. Using polynomial regression and response surface analyses, we find that in contexts with a longer introductory period (i.e., higher expectation maturity), disconfirmation has a nonlinear relationship with trusting intention. When the introductory period is shorter (i.e., expectation maturity is lower), disconfirmation has a linear relationship with trusting intention. This unique set of empirical findings shows when it is valuable to use nonlinear modeling for understanding technology trust disconfirmation. We conclude with implications for future research.  
Extant research has focused on the detection of fake reviews on online review platforms, motivated by the well-documented impact of customer reviews on the users' purchase decisions. The problem is typically approached from the perspective of protecting the credibility of review platforms, as well as the reputation and revenue of the reviewed firms. However, there is little examination of the vulnerability of individual businesses to fake review attacks. This study focuses on formalizing the visibility of a business to the customer base and on evaluating its vulnerability to fake review attacks. We operationalize visibility as a function of the features that a business can cover and its position in the platform's review-based ranking. Using data from over 2.3 million reviews of 4,709 hotels from 17 cities, we study how visibility can be impacted by different attack strategies. We find that even limited injections of fake reviews can have a significant effect and explore the factors that contribute to this vulnerable state. Specifically, we find that, in certain markets, 50 fake reviews are sufficient for an attacker to surpass any of its competitors in terms of visibility. We also compare the strategy of self-injecting positive reviews with that of injecting competitors with negative reviews and find that each approach can be as much as 40% more effective than the other across different settings. We empirically explore response strategies for an attacked hotel, ranging from the enhancement of its own features to detecting and disputing fake reviews. In general, our measure of visibility and our modeling approach regarding attack and response strategies shed light on how businesses that are targeted by fake reviews can detect and tackle such attacks.  
Organizational agility is a significant business capability. Though there have been numerous studies about the effects of information technology (IT) capabilities on organizational agility, there has been limited attention on the enabling effects of IT ambidexterity, namely, the dual capacity to explore and exploit IT resources and practices. We propose that IT ambidexterity enhances organizational agility by facilitating operational ambidexterity, and that the magnitude of facilitation depends on the level of environmental dynamism. We test these relationships utilizing data from a large-scale, matched-pair field survey of business and IT executives. The results confirm that a firm's IT ambidexterity does enhance its organizational agility through the mediated effects of operational ambidexterity, and that the dynamism of a firm's environment affects these relationships.  
Health information technology has increased accessibility of health and medical data and benefited medical research and healthcare management. However, there are rising concerns about patient privacy in sharing medical and healthcare data. A large amount of these data are in free text form. Existing techniques for privacy-preserving data sharing deal largely with structured data. Current privacy approaches for medical text data focus on detection and removal of patient identifiers from the data, which may be inadequate for protecting privacy or preserving data quality. We propose a new systematic approach to extract, cluster, and anonymize medical text records. Our approach integrates methods developed in both data privacy and health informatics fields. The key novel elements of our approach include a recursive partitioning method to cluster medical text records based on the similarity of the health and medical information and a value-enumeration method to anonymize potentially identifying information in the text data. An experimental study is conducted using real-world medical documents. The results of the experiments demonstrate the effectiveness of the proposed approach.  
To manage work interdependencies, online communities draw on a variety of arm's length coordination mechanisms offered by information technology platforms and associated practices. However, "unresolved interdependencies" remain that cannot be addressed by such arm's length mechanisms. These interdependencies reflect, for example, unidentified or emerging knowledge-based dependencies between the community members or unaccounted relationships between ongoing community tasks. At the same time, online communities cannot resort to hierarchical coordination mechanisms such as incentives or command structures to address such interdependencies. So, how do they manage such interdependencies? To address this question, we conduct an exploratory, theory-generating case study involving qualitative and computational analyses of development activities within an open source software community: Rubinius. We analyze the ongoing management of interdependencies within the community and find that unresolved interdependencies are associated with alternatively structured sequences of activities, which we define as routines. In particular, we observe that two distinct classes of interdependencies--development and developer interdependencies--are associated with alternative forms of routine variation. We identify two generalized routine components--direct implementation and knowledge integration, which address these two distinct classes of unresolved interdependencies. In particular, direct implementation deals with development interdependencies within the code that are not already coordinated through modular interfaces, while knowledge integration resolves unaccounted interdependencies between developers. We conclude with implications for research into organizing principles for online communities and note the significance of our findings for the study of coordination in organization studies in general.  
The dramatic increase in social media use has challenged traditional social structures and shifted a great deal of interpersonal communication from the physical world to cyberspace. Much of this social media communication has been positive: Anyone around the world who has access to the Internet has the potential to communicate with and attract a massive global audience. Unfortunately, such ubiquitous communication can be also used for negative purposes such as cyberbullying, which is the focus of this paper. Previous research on cyberbullying, consisting of 135 articles, has improved the understanding of why individuals--mostly adolescents--engage in cyberbullying. However, our study addresses two key gaps in this literature: (1) how the information technology (IT) artifact fosters/inhibits cyberbullying and (2) why people are socialized to engage in cyberbullying. To address these gaps, we propose the social media cyberbullying model (SMCBM), which modifies Akers' [Akers RL (2011) Social Learning and Social Structure: A General Theory of Crime and Deviance, 2nd ed. (Transaction Publishers, New Brunswick, NJ)] social structure and social learning model. Because Akers developed his model for crimes in the physical world, we add a rich conceptualization of anonymity composed of five subconstructs as a key social media structural variable in the SMCBM to account for the IT artifact. We tested the SMCBM with 1,003 adults who have engaged in cyberbullying. The empirical findings support the SMCBM. Heavy social media use combined with anonymity facilitates the social learning process of cyberbullying in social media in a way that fosters cyberbullying. Our results indicate new directions for cyberbullying research and implications for anticyberbullying practices.  
Social networks have been shown to affect health. Because online social networking makes it easier for individuals to interact with experientially similar others in regard to health issues and to exchange social support, there has been an increasing effort to understand how networks function. Nevertheless, little attention has been paid to how these networks are formed. In this paper, we examine the driving forces behind patients' social network formation and evolution. We argue that patients' health-related traits influence their social connections and that the patients' network layout is shaped by their cognitive capabilities and their network embeddedness. By studying longitudinal data from 1,322 individuals and their communication ties in an online healthcare social network, we find that firsthand disease experience, which provides knowledge of the disease, increases the probability that patients will find experientially similar others and establish communication ties. Patients' cognitive abilities, including the information load that they can process and the range of social ties that they can manage, however, limit their network growth. In addition, we find that patients' efforts to reach out for additional social resources are associated with their embeddedness in the network and the cost of maintaining connections. Practical implications of our findings are discussed.  
Software as a Service (SaaS) delivers a bundle of applications and services through the Web. Its on-demand feature allows users to enjoy full scalability and to handle possible demand fluctuations at no risk. In recent years, SaaS has become an appealing alternative to purchasing, installing, and maintaining modifiable off-the-shelf (MOTS) software packages. We present a game-theoretical model to study the competitive dynamics between the SaaS provider, who charges a variable per-transaction fee, and the traditional MOTS provider. We characterize the equilibrium conditions under which the two coexist in a competitive market and those under which each provider will fail and exit the market. Decreasing the lack-of-fit (or the cross-application data integration) costs of SaaS results in four structural regimes in the market. These are MOTS Dominance→Segmented Market→Competitive Market→SaaS Dominance. Based on our findings, we recommend distinct competitive strategies for each provider. We suggest that the SaaS provider should invest in reducing both its lack-of-fit costs and its per-transaction price so that it can offer increasing economies of scale. The MOTS provider, by contrast, should not resort to a price-cutting strategy; rather, it should enhance software functionality and features to deliver superior value. We further examine this problem from the software life-cycle perspective, with multiple stages over which users can depreciate the fixed costs of installing and customizing their MOTS solutions on site. We then present an analysis that characterizes the competitive outcomes when future technological developments could change the relative levels of the lack-of-fit costs. Specifically, we explain why the SaaS provider will always use a forward-looking pricing strategy: When lack-of-fit costs are expected to decrease (increase) in the future, the SaaS provider should reduce (increase) its current price. This is in contrast with the MOTS provider, who will use the forward-looking pricing strategy only when lack-of-fit costs are expected to increase. Surprisingly, when such costs are expected to decrease, the MOTS provider should ignore this expectation and use the same pricing strategy as in the benchmark with invariant lack-of-fit costs.  
Chronic excessive turnover among information technology (IT) professionals has been costly to firms for decades with annual turnover rates as high as 24% even among Computerworld's '100 Best Places to Work in IT.' Prior information systems literature has identified two key factors affecting turnover: boundary-spanning roles and low promotability in one's current firm. We draw on tournament theory, which is primarily concerned with inducing effort in employees, to decompose promotability into two distinct constructs: the likelihood of promotion and benefit from promotion, and demonstrate that each has a distinct role in affecting turnover rates. Our key result is that a job ladder motivating IT professionals with large, infrequent promotions will lead to higher turnover than a job ladder with smaller, more frequent promotions. We describe the conditions under which rearranging the job ladder creates economic value for the firm. We also offer an explanation for the observation that jobs characterized by boundary-spanning activities have higher turnover, and show that such jobs are more sensitive to the effect of likelihood of promotion on turnover. We test our hypotheses on a detailed data set covering 5,704 IT professionals over a five-year period. We confirm that likelihood of promotion has the predicted effects on turnover of IT professionals. A one standard deviation increase in likelihood of promotion decreases turnover by over 99%, consistent with our prediction. The empirical analysis also confirms the predicted effects of boundary spanning activities.  
There is substantial research on the effects of formal control structures (i.e., incentives, identities, organization, norms) on knowledge sharing leading to innovative outcomes in online communities. However, there is little research on how knowledge-sharing trajectories in temporary online crowds create innovative outcomes without these structures. Such research is particularly of interest in the context of temporary online crowds solicited with crowdsourcing in which there is only minimal structure for knowledge sharing. We identify eight types of crowdsourcing with different knowledge-sharing patterns. The focus of this study is on the one type of crowdsourcing--collaborative innovation challenges--in which there is the least restriction on knowledge sharing in the crowd. A content analysis was conducted of all time-stamped posts made in five different collaborative innovation challenges to identify different knowledge-sharing trajectories used. We found that a paradox-framed trajectory was more likely to be followed by innovative outcomes compared to three other knowledge-sharing trajectories. A paradox-framed trajectory is one in which a novel solution emerges when different participants post in the following sequence: (1) contributing a paradox associated with the problem objective, (2) sharing assumptions to validate the paradox, and (3) sharing initial ideas for resolving the paradox in a manner that meets the problem statement. Based on the findings, a theory of paradox-framed trajectories in temporary online crowds is presented along with implications for knowledge creation theories in general and online knowledge-creating communities in particular.  
Pay-per-click (PPC) is a common pricing model used to pay for ads on the Web and is open to the possibility for click fraud, where clicks are not from a legitimate user. Identifying click fraud is generally done in a three-stage process: the service provider (SP) first classifies clicks as fraudulent or not, then the advertiser does the same with a different technology, and if there is a disagreement, the SP examines further and his conclusions are considered binding. The advertiser pays for clicks that are identified as valid in the first two stages or confirmed as valid in the last stage. We model the choice of the identification technologies as a double moral hazard problem. We analyze the case where the PPC is incentive compatible to overcome the moral hazard problem, and examine the question of whether the incentive compatible PPC is sufficient to incentivize the two parties to unilaterally make further improvements to their identification technologies and simultaneously increase their profits. We show that when the cost of the third-stage identification technology is large, which is likely to be the case because of its complexity and use of expensive human experts, the incentive compatible PPC does not support unilateral technological improvements. We then examine a setting where the third-stage identification is delegated to a third party and find that this arrangement can induce unilateral improvements to the identification technologies in the first two stages. Collectively our results show that although the PPC model itself may not induce improvements in the first two stages of click fraud identification, a common arrangement espoused of having a third party resolve disagreements helps make PPC support unilateral technological improvements. Accordingly, we show an indirect benefit to the third-party arrangement.  
How does information technology (IT) enable firms to globalize their operations and achieve higher foreign profits? We use archival data for multinational firms publicly traded in the United States for the years 1999-2006, and find indirect evidence for the role of IT to help firms achieve higher foreign profits through revenue growth rather than cost reduction. Our findings suggest that foreign responsiveness plays a more important role in generating foreign profits than does value chain structure. Our exploratory analyses for the effect of IT on domestic revenues and profits suggest some evidence for equalization of returns across foreign and domestic operations. Among additional results, we find that research and development is positively associated with foreign revenues and foreign profits with an effect greater than that of IT, and advertising is positively associated with foreign revenues with an effect greater than that of IT. By documenting how IT creates value for firms through globalization, we extend the business value of IT and international business literatures that have so far touched on firm-level globalization benefits from IT only in passing. The findings can help managers decide how to allocate discretionary expenditures to achieve strategic objectives such as foreign and domestic revenues and profits, and the role of revenue versus cost mechanisms. The online appendix is available at .  
This paper studies the effect of aggregate information technology (IT) investments on customer satisfaction and profits at the firm level. Using data on 109 U.S. firms for the 1994-1996 and 1999-2006 periods, we find that aggregate IT investments have a positive association with customer satisfaction. However, the strength of the relationship varied across the 1994-1996 and 1999-2006 periods. Specifically, IT investments had a more positive influence on customer satisfaction for the 1994-1996 period than for the 1999-2006 period. Conversely, IT investments had a positive effect on profits in the 1999-2006 period, but a negative effect in the 1994-1996 period. These findings extend prior discourse in the information systems literature on the role of customer satisfaction as a mechanism that explains how IT-enabled benefits are "passed on to consumers" [Rai A, Patnayakuni R, Patnayakuni N (1997) Technology investment and business performance. Comm. ACM 40(7):90]. Our additional exploratory analyses showing that IT investments had a stronger effect on perceived quality than on perceived value provide an explanation for some of the observed effects of IT on customer satisfaction and profits. Together, these contributions and implications provide new insights to assess returns on IT investments by focusing on customer satisfaction, an important intangible and leading measure of firm performance, stock returns, and stock risk.  
With the nearly instantaneous dissemination of information in the modern era, policies regarding the disclosure of sensitive information have become the focus of significant discussion in several contexts. The fundamental debate centers on trade-offs inherent in disclosing information that society needs, but that can also be used for nefarious purposes. Using information security as a research context, our empirical study examines the adoption of software vulnerabilities by a population of attackers. We compare attacks based on software vulnerabilities disclosed through full-disclosure and limited-disclosure mechanisms. We find that full disclosure accelerates the diffusion of attacks, increases the penetration of attacks within the target population, and increases the risk of first attack after the vulnerability is reported. Interestingly, the effect of full disclosure is greater during periods when there are more overall vulnerabilities reported, indicating that attackers may strategically focus on busy periods when the effort of security professionals is spread across many vulnerabilities. Although the aggregate volume of attacks remains unaffected by full disclosure, attacks occur earlier in the life cycle of the vulnerability. Building off our theoretical insights, we discuss the implications of our findings in more general contexts.  
Emerging from rapid advances in digitization and technological capabilities is a new form of information systems development project: cyber projects. Cyber projects are complex, massive, and ambitious, often involving hundreds of academic, government, and industry professionals, requiring years of development, and costing millions of dollars. In our study, we examine how control is exercised in cyber projects. Based on a longitudinal study over eight years, we develop a process theory of the control of cyber projects. Initially we observe that project control is driven by the field, i.e., all of the individual or collective entities that subscribe to the general purpose of the project. This form of control is later replaced by a more bureaucratic form from government-sponsored entities to ensure that traditional project objectives are met. Once construction begins and the field understands the implications and promise of the project, we observe that control is again exerted by the primary project users in the field, complemented by authority-based control exerted by the government-sponsored entisty in the field.  
From the perspective of leader-member exchange theory, we investigate how two forms of leadership style (uniform leader-member exchange (ULMX) and differential leader-member exchange (DLMX)) impact member participation in online collaborative work communities (OCWC). Furthermore, based on computer simulations, we also examine the moderating impact of key contextual factors on the relationship between leadership style and member contributions. Efficacy of leadership style in OCWCs is greatly influenced by environmental conditions. DLMX is more effective in sustaining member commitment under high environmental uncertainty, regardless of network size and structure. ULMX is more effective in decentralized structures and during the early stage of community growth. The simulation-based insights suggest that supervisory behavior does matter to member retention and sustained participation in OCWCs, but its impact is significantly moderated by many contextual factors, such as community size, structure, maturity, and environmental uncertainty. In certain situations ULMX prevails, but in others DLMX is more effective. These two forms of governance in fact complement each other, rather than being mutually exclusive forms of leadership style. To attain a maximal outcome, leaders should flexibly adapt their governance styles between DLMX and ULMX over the life cycle of an OCWC to maximize member retention and performance benefits.  
This study explores the role of social media in social change by analyzing Twitter data collected during the 2011 Egypt Revolution. Particular attention is paid to the notion of collective sense making, which is considered a critical aspect for the emergence of collective action for social change. We suggest that collective sense making through social media can be conceptualized as human-machine collaborative information processing that involves an interplay of signs, Twitter grammar, humans, and social technologies. We focus on the occurrences of hashtags among a high volume of tweets to study the collective sense-making phenomena of milling and keynoting. A quantitative Markov switching analysis is performed to understand how the hashtag frequencies vary over time, suggesting structural changes that depict the two phenomena. We further explore different hashtags through a qualitative content analysis and find that, although many hashtags were used as symbolic anchors to funnel online users' attention to the Egypt Revolution, other hashtags were used as part of tweet sentences to share changing situational information. We suggest that hashtags functioned as a means to collect information and maintain situational awareness during the unstable political situation of the Egypt Revolution.  
Information technologies (IT) act as an enabler for policy implementation in the U.S. federal government. While federal agencies increasingly rely on advanced digital technologies to execute new policy initiatives, many agencies are struggling with maintaining decades old legacy systems. This study investigates how national politics affects IT investment profiles in U.S. federal agencies. Drawing on a range of literature from the political science, public administration, and information systems (IS) disciplines, we hypothesize that a federal agency's capacity-building IT investments are associated with (i) legislative approval for the chief executive, (ii) government dividedness, and (iii) the agency's ideological characteristic. With a panel data set from 135 federal agencies and bureaus in 2003-2016, our empirical analyses produce several intriguing findings. For instance, when the U.S. Senate and the House of Representatives are controlled by the President's ruling party, federal agencies are predicted to invest approximately 8.32% more in new IT development and modernization than when the opposition party holds the majority in both chambers. We contribute to the IS literature by demonstrating that budget allocation decisions between IT development and maintenance in governments are affected by political environments. We also offer several policy prescriptions in IT management for policymakers and practitioners in the public sector.  
Most of the work on context-aware recommender systems has focused on demonstrating that the contextual information leads to more accurate recommendations. Little work has been done, however, on studying how much the contextual information affects the business performance. In this paper, we study how including context in recommendations affects customers' trust, sales, and other crucial business-related performance measures. To do this, we delivered content-based and context-aware recommendations through a live controlled experiment with real customers of a commercial European online publisher. We measured the recommendations' accuracy and diversification, how much customers spent purchasing products during the experiment, the quantity and price of their purchases, and the customers' level of trust. We show that collecting and using contextual information in recommendations affects business-related performance measures, such as company sales, by improving the accuracy and diversification of recommendations, which in turn improves trust and, ultimately, business performance results.  
Competitive isomorphism refers to the phenomenon of competing firms becoming similar as they mimic each other under common market forces. With the growing presence of firms as well as their consumers and suppliers on the Web, we discover a parallel phenomenon of online isomorphism wherein the Web footprints of competing firms are found to overlap. We propose new online metrics based on the content, in-links, and outlinks of firms' websites to measure the presence of online isomorphism as well as uncover its utility in predicting competitor relationships. Through rigorous analysis involving more than 2,600 firms, we find that predictive models for competitor identification based on online metrics are largely superior to those using offline data such as Standard Industrial Classification codes and market values of firms. In addition, combining online and offline metrics can boost the predictive performance. We also find that such models are valuable for identifying nuances of competitor relationships such as asymmetry and the role of industrial divisions. Furthermore, the suggested predictive models can effectively rank firms in an industrial division by their likelihood of being competitors to a focal firm as well as identify new future competitors, thus adding to a portfolio of evidence indicating their utility for managers and analysts.  
This paper focuses on finding the same and similar users based on location-visitation data in a mobile environment. We propose a new design that uses consumer-location data from mobile devices (smartphones, smart pads, laptops, etc.) to build a "geosimilarity network" among users. The geosimilarity network (GSN) could be used for a variety of analytics-driven applications, such as targeting advertisements to the same user on different devices or to users with similar tastes, and to improve online interactions by selecting users with similar tastes. The basic idea is that two devices are similar, and thereby connected in the GSN, when they share at least one visited location. They are more similar as they visit more shared locations and as the locations they share are visited by fewer people. This paper first introduces the main ideas and ties them to theory and related work. It next introduces a specific design for selecting entities with similar location distributions, the results of which are shown using real mobile location data across seven ad exchanges. We focus on two high-level questions: (1) Does geosimilarity allow us to find different entities corresponding to the same individual, for example, as seen through different bidding systems? And (2) do entities linked by similarities in local mobile behavior show similar interests, as measured by visits to particular publishers? The results show positive results for both. Specifically, for (1), even with the data sample's limited observability, 70%-80% of the time the same individual is connected to herself in the GSN. For (2), the GSN neighbors of visitors to a wide variety of publishers are substantially more likely also to visit those same publishers. Highly similar GSN neighbors show very substantial lift.  
The software development field has inherently been identified with two field-level institutional logics, i.e., logic of the profession and logic of the markets. Traditionally, information systems development methodologies have been used to reconcile the competing demands from these two logics. In this paper, we study how these two logics manifest in a platform-based software ecosystem where significant entrepreneurial opportunities are created for independent third-party app developers ( indies). Specifically, we study how indie developers manage the two logics on the iOS platform ecosystem under the influence of the platform owner Apple. We first identify practices of indie developers consistent with the two field logics across three entrepreneurial domains, i.e., app ideation, execution, and marketing. Second, we identify areas where the two field logics may be in conflict as well as in coexistence. We show that indie developers enact logic conflict through disagreement and denunciation of the opposing logic. When logics that are inherently opposed appear to coexist, we investigate how app developers manage these opposing demands through a process we call logic synthesis. Using a grounded theory approach, we propose a model of field-level market and professional logics operating in the mobile app platform ecosystem and the practices reflecting such logics in the indie developer community. Our work contributes to the growing literature on platform ecosystems and the processes adopted by third-party app developers in such ecosystems, in addition to furthering the study of institutional logics in technology contexts.  
While information technology benefits society in numerous ways, it unfortunately also has potential to create new vulnerabilities. This special issue intends to stimulate thought and research into understanding and mitigating these vulnerabilities. We identify four mechanisms by which ubiquitous computing makes various entities (people, devices, organizations, societies, etc.) more vulnerable, including: increased visibility, enhanced cloaking, increased interconnectedness, and decreased costs. We use the papers in the special issue to explain these mechanisms, and then outline a research agenda for future work on digital vulnerabilities spanning four areas that are, or could become, significant societal problems with implications at multiple levels of analysis: Online harassment and incivility, technology-driven economic inequality, industrial Internet of Things, and algorithmic ethics and bias.  
This paper presents new evidence on the role of embeddedness in predicting contract duration in the context of information technology outsourcing. Contract duration is a strategic decision that aligns interests of clients and vendors, providing the benefits of business continuity to clients and incentives to undertake relationship specific investments for vendors. Considering the salience of this phenomenon, there has been limited empirical scrutiny of how contract duration is awarded. We posit that clients and vendors obtain two benefits from being embedded in an interorganizational network. First, the learning and experience accumulated from being embedded in a client-vendor network could mitigate the challenges in managing longer term contracts. Second, the network serves as a reputation system that can stratify vendors according to their trustworthiness and reliability, which is important in longer term arrangements. In particular, we attempt to make a substantive contribution to the literature by theorizing about embeddedness at four distinct levels: structural embeddedness at the node level, relational embeddedness at the dyad level, contractual embeddedness at the level of a neighborhood of contracts, and finally, positional embeddedness at the level of the entire network. We analyze a data set of 22,039 outsourcing contracts implemented between 1989 and 2008. We find that contract duration is indeed associated with structural and positional embeddedness of participant firms, with the relational embeddedness of the buyer-seller dyad, and with the duration of other contracts to which it is connected through common firms. Given the nature of our data, identification using traditional ordinary least squares based approaches is difficult given the unobserved errors clustered along two nonnested dimensions and the autocorrelation in a firm's decision (here the contract) with those of contracts in its reference group. We use a multiway cluster robust estimation and a network auto-regressive estimation to address these issues. Implications for literature and practice are discussed.  
Although control beliefs (CBs) can represent many different types of controls, information systems researchers have focused primarily on CBs related to technical compatibility, resource availability, and computer self-efficacy. More recent research has recognized that co-worker advice, which represents situated and improvised learning, can also be an important factor that can enable or impede system use. In addition, because advice from co-workers represents the social context by which the impacts of other traditional CBs are embedded, they may have the potential to alter the relationships between traditional CBs and system use. Against this backdrop, we examined the direct effects of CBs about advice from co-workers on system use as well as its ability to moderate the effects of other types of CBs on system use. To accomplish this, we conducted a three-month study of 112 employees in one business unit of an organization. Results supported our hypotheses that CBs about advice from co-workers directly influence system use and moderate the effects of other CBs on system use.  
Information technology (IT) services vendors operate in a highly competitive but also institutional environment that render their service-line offerings mutually observable. This suggests that imitation of rivals' decisions can be an efficient means for IT vendors when reconfiguring their service-line offerings. To explore how such imitation unfolds in this sector, we estimate a series of logistic regression models of 116 IT vendors' serviceline choices over three time periods. First, from the strategic imitation literature we identify the key imitation "referents," which is a group of firms or a single firm with specific traits, and we test the relative influence of each referent. All of our analysis includes these referents as predictors of service-line choice. Next, we tested more nuanced models using theoretically guided subsamples as follows. One, based on information systems (IS) literature, we consider the IT vendors as embedded in three distinct "institutional spheres," each corresponding to a knowledge domain, namely, technical, functional, and vertical industry domains. We separately examine imitation in each subsample corresponding to the three types of service lines. Two, based on strategy literature, we consider that the influence of the imitation referents differs when the choice under consideration is the addition of a new service line versus a withdrawal. Our results across all of these subsamples uncover a nuanced pattern of imitation that sometimes contrasts the full-sample results. The most prominent result is that although imitation is highly salient, the different imitation referents are not universally influential across all knowledge domains and between development versus withdrawal decisions. Specifically, the imitation of similar firms is widespread, whereas the imitation of largest firms or offering popular service-lines, which indicates bandwagon effects, are at play only selectively. This study contributes to the IS literature by laying a basis for a variety of research directions including resource spillovers and vicarious learning in IT sectors.  
Academics and practitioners alike recognize that user-generated content (UGC), such as blog posts, help not only predict but also boost performance (e.g., sales). However, the role of competition in the UGC domain is not well understood. Building on extant research pertaining to the UGC-performance relationship, the authors document empirical evidence for a relationship between competitor UGC and focal firm performance. Data from a 30-week period describe the viewership of competing cable news shows on Fox News, CNN, and MSNBC during the 7:00 P.M.-9:00 P.M. time slots. They find evidence of a statistically significant relationship between competitor UGC and viewership and of heterogeneity in the direction of these competitive relationships, positive in some time slots and negative in others. The predictive power of UGC for viewership is enhanced by 3% to 5% simply by incorporating competitors' UGC, in addition to a show's own UGC. Thus, the study, as well as formulation of UGC-related marketing strategies, should incorporate competitive relationships.  
Gamification, an application of game design elements to non-gaming contexts, is proposed as a way to add engagement in technology-mediated training programs. Yet there is hardly any information on how to adapt game design elements to improve learning outcomes and promote learner engagement. To address this issue, we focus on a popular game design element, competition, and specifically examine the effects of different competitive structures, i.e., whether a person faces a higher-skilled, lower-skilled or equally-skilled competitor, on learning and engagement. We study a gamified training design for databases, where trainees play a trivia-based mini-game with a competitor after each e-training module. Trainees who faced a lower-skilled competitor reported higher self-efficacy beliefs and better learning outcomes, supporting the effect of peer appraisal, a less examined aspect of social cognitive theory. Yet trainees who faced equally-skilled competitors reported higher levels of engagement, supporting the balance principle of flow theory. Our study findings indicate that no one competitive structure can simultaneously address learning and engagement outcomes. The choice of competitive structures depends on the priority of the outcomes in training. Our findings provide one explanation for the mixed findings on the effect of competitive gamification designs in technology mediated training.  
Jump bidding, which refers to bidding above the minimum necessary, is a robust behavior that has been observed in a variety of ascending auctions in the field as well as the laboratory. However, the phenomenon has yet to be studied in combinatorial auctions, which are a type of multiobject auction that allows bidders to bid on a set of objects. Such auctions have been found to be beneficial when objects exhibit synergy, e.g., are complementary. In this paper, we explore jump bidding behavior in combinatorial auctions as a function of design choices of the mechanism. In particular, we examine the effects of price revelation schemes on the nature and extent of jump bidding. Furthermore, we study the effects of jump bidding on the economic performance of the auctions. To conduct our study, first, we develop hypotheses using auction theories and behavioral theories of how people use reference prices as anchors, and second, we conduct a laboratory experiment to test our hypotheses and examine bidder behavior. We find that the nature of the prices that the auctioneer chooses to offer as feedback to the bidders can considerably influence their jump bidding behavior, leading to significant differences in auction outcomes. We demonstrate that in combinatorial auctions, in addition to the theories of jump bidding proposed in the literature, bounded rationality of the bidders plays a part in the nature and extent of jump bidding. Our study reveals that in the cognitively challenging package-bidding environment, bidders often pursue computationally frugal but suboptimal heuristics. Our results have important policy implications for mechanism designers.  
Open source communities rely on the espoused premise of complete openness and transparency of source code and development process. Yet, openness and transparency at times need to be balanced out with moments of less open and transparent work. Through our detailed study of Linux Kernel development, we build a theory that explains that transparency and openness are nuanced and changing qualities that certain developers manage as they use multiple digital technologies and create, in moments of needs, more opaque and closed digital spaces of work. We refer to these spaces as digital folds. Our paper contributes to the extant literature by providing a process theory of how transparency and openness are balanced with opacity and closure in open source communities according to the needs of the development work; by conceptualizing the nature of digital folds and their role in providing quiet spaces of work; and, by articulating how the process of digital folding and unfolding is made far more possible by select elite actors' navigating the line between the pragmatics of coding and the accepted ideology of openness and transparency  
Although most behavioral security studies focus on organizational in-role behaviors such as information security policy (ISP) compliance, the role of organizational extra-role behaviors—security behaviors that benefit organizations but are not specified in ISPs—has long been overlooked. This study examines (1) the consequences of organizational in-role and extra-role security behaviors on the effectiveness of ISPs and (2) the role of formal and social controls in enhancing in-role and extra-role security behaviors in organizations. We propose that both in-role security behaviors and extra-role security behaviors contribute to ISP effectiveness. Furthermore, based on social control theory, we hypothesize that social control can boost both in- and extrarole security behaviors. Data collected from practitioners—including information systems (IS) managers and employees at many organizations—confirmed most of our hypotheses. Survey data from IS managers substantiated the importance of extra-role behaviors in improving ISP effectiveness. Paired data, collected from managers and employees in the same organizations, indicated that formal control and social control individually and interactively enhance both in- and extra-role security behaviors. We conclude by discussing the implications of this research for academics and practitioners, along with compelling future research possibilities.  
Literature has identified factors such as piracy, network externality, or concave cost of producing quality as key drivers of software versioning. However, software firms adopt versioning strategies that are often invariant across different market settings. To explain universal business practice of software versioning, we focus on "inconvenience" or disutility that users experience when software has lower functionality than what they require to accomplish tasks. In our model, users are heterogeneous on marginal valuation for functionality and the required level of functionality such that those with higher valuation have a higher required level of functionality. Users do not derive any additional utility if the software has more functionality than what they require. We show that heterogeneous disutility from underprovisioning of functionality is a sufficient condition for optimality of versioning under fairly general conditions. We also show that, as high-type users' required level of functionality increases, the firm increases the functionality level of the high version. Yet surprisingly, the firm may decrease the functionality level of the low version if the proportion of high-type users is moderate. On the other hand, as the required level of functionality of low-type users increases, the firm may reduce the functionality level of the low version when the proportion of high-type users is high, though the functionality level of the high version remains the same. Counterintuitively, an increase in the high-type (low-type) users' required level of functionality negatively (positively) impacts high-type users' consumer surplus.  
Voluntary contributions are crucial to the success of open source software (OSS) projects. Firms sponsoring OSS projects may face substantial challenges in soliciting such contributions, since volunteer participants are neither regulated by an employment contract nor offered financial incentives. Although prior work has shown the positive impact of motivation on the effort expended by volunteer participants, there is limited understanding of how specific firm attributes shape volunteers' intrinsic motivation. We offer a theoretical model of how the perceived community-based credibility and openness of the sponsoring firm have a positive impact on the intrinsic motivation of volunteer participants. The model is explored using survey data on volunteer participants from two sponsored OSS projects. Results show that a sponsoring firm's community-based credibility (OSS developers' perception of its expertise and trustworthiness) and openness (its mutual knowledge exchange with the community) strengthen the volunteer participants' social identification with the firm-sponsored community, which in turn reinforces their intrinsic motivation to participate. Moreover, the perceived community-based credibility of a sponsoring firm directly enhances volunteer participants' intrinsic motivation, whereas perceived openness fails to affect motivation without the mediating mechanism of social identification. Implications for firms seeking voluntary contributions for their sponsored OSS projects are discussed.  
Within online innovation communities, remixing (i.e., the community's use of an existing innovation as source material or inspiration to aid in the development of further innovations) is an interesting form of knowledge collaboration. This study investigates an open theoretical question: Why are particular innovations remixed by online innovation communities? Some innovations languish, while others are widely remixed. Community members (even those unknown to the innovation's creator) may remix, taking the source innovation in directions the original innovator may have never imagined. Within online innovation communities, remixing is not bound by some of the constraints to knowledge collaboration found in more traditional environments. To address our research question, we begin with variables constituent to innovation diffusion theory, essentially remixing this long-established theory to predict cumulative remixing in online innovation communities, using arguments grounded in the user innovation and learning literatures. We also consider two forms of communication that are relevant to knowledge sharing in online communities (online community interaction and front page presence). Regression analysis (using data pertaining to 498 3D printable innovations) shows that community interaction is highly influential in determining which innovations are remixed by the community. Conversely, the innovation having a presence on the community's front page does not have a significant effect on remixing. Observability has an inverse-U-shaped relationship with remixing; this suggests the value placed on experiential learning. Fuzzy set qualitative comparative analysis (fsQCA) is used as a supplementary analysis technique (with robustness testing), and the results are largely consistent with regression findings but offer interesting insight into innovation configurations that consistently result in remixing from the community. Within specific configurations, fsQCA results indicate a contingent effect of observability and complexity; that is, under certain conditions, complex innovations are more likely to be remixed by the community.  
It is not enough to get information technology (IT) users to adopt a secure behavior. They must also continue to behave securely. Positive outcomes of secure behavior may encourage the continuance of that behavior, whereas negative outcomes may lead users to adopt less-secure behaviors. For example, in the context of authentication, login success rates may determine whether users continue to use a strong credential or switch to less secure behaviors (e.g., storing a credential or changing to a weaker, albeit easier to successfully enter, credential). Authentication is a particularly interesting security behavior for information systems researchers to study because it is affected by an IT artifact (the design of the user interface). Laptops and desktop computers use full-size physical keyboards. However, users are increasingly adopting mobile devices, which provide either miniature physical keypads or touchscreens for entering authentication credentials. The difference in interface design affects the ease of correctly entering authentication credentials. Thus, the move to use of mobile devices to access systems provides an opportunity to study the effects of the user interface on authentication behaviors. We extend existing process models of secure behaviors to explain what influences their (dis)continuance. We conduct a longitudinal field experiment to test our predictions and find that the user interface does affect login success rates. In turn, poor performance (login failures) leads to discontinuance of a secure behavior and the adoption of less-secure behaviors. In summary, we find that a process model reveals important insights about how the IT artifact leads people to (dis)continue secure behaviors.  
Electronic commerce can improve market efficiency by helping buyers and sellers find and transact with each other across geographic distance. We study the effect of two distinct forms of electronic commerce on market efficiency, which we measure via the existence and exploitation of spatial arbitrage opportunities. Spatial arbitrage represents a more precise measure of market efficiency than does price dispersion, which is typically used, because it accounts for the transaction costs of trading across distance and for unobserved product heterogeneity. One of the forms of electronic commerce that we study is a webcast channel that allows electronic access to the traditional physical market, while the other is a standalone electronic market. Both forms provide traders with expanded reach to find and transact with each other across geographic distance, but only one allows traders to conduct transactions immediately, at any time. This variance helps us isolate the effect of these mechanisms (reach and transaction immediacy) on efficiency. We find that electronic commerce reduces the number of arbitrage opportunities (likely by expanding traders' geographic reach) but improves arbitrageurs' ability to identify and exploit those that remain (likely by expanding arbitrageurs' reach and providing them with the immediacy to exploit opportunities quickly). Overall, our results suggest that electronic commerce improves market efficiency not only by helping buyers and sellers transact across distance (thereby balancing supply and demand across geographic locations) but also by helping arbitrageurs quickly exploit any remaining arbitrage opportunities (thereby rebalancing supply and demand across geographic locations).  
In the paradigm of service-centric computing, new value-added applications can be developed dynamically and flexibly by combining and integrating existing services. While software applications are traditionally specified and implemented as a set of functions uniform to all users, this new paradigm allows the same software service to be delivered with a different price, response time, availability, and other nonfunctional attributes to accommodate different modes of use. These nonfunctional attributes together are referred to as Quality of Service (QoS). When creating a new composite service, negotiation makes it possible for a service provider to offer the service with the QoS properties customized to the needs of a user. Automated negotiation tactics require the specification of reservation values for the QoS attributes. We present a methodology that determines the reservation values a user (or broker) should use for each component service based on the user's minimum requirements for the composite service. Our methodology maximizes the chance of reaching a successful negotiation outcome while staying within the user's reservation values for the composite service. We show that the problem of determining the user's reservation values for component services can be modeled as a multiobjective optimization problem and then transformed to a single-objective optimization problem using a max-min approach. The formulation can incorporate providers' different QoS preferences to increase the chance of negotiation success. We identify problem instances for which closed-form solutions can be found for the reservation values. We show how the method of setting reservation values can be incorporated into a negotiation process that uses extant concession and trade-off tactics. Simulation experiments demonstrate the effectiveness of the proposed approach. If some providers accept offers before the negotiation process deadline, we show that dynamically changing the reservation values for the remaining providers makes the overall negotiation process more likely to succeed. The online appendix is available at .  
Early studies on Web design typically caution against the use of distracting website features in electronic commerce, such as animated banners, pop-ups, and floating advertisements, because they may cause annoyance for online consumers and disrupt information processing, leading to poorer purchase decisions. Yet, the recently uncovered deliberation-without-attention (D-W-A) effect suggests that distracting consumers from the decision-making process may improve their decision quality when there are a large number of decision parameters to consider. To ascertain whether the D-W-A effect can be triggered through the use of distracting website features in the context of online shopping, two experiments are conducted. The first experiment reveals that the presence of distracting website features, in the form of pop-ups, gives rise to annoyance in general, but also leads to better purchase decisions when the decision to be made is complex. The second experiment supports the findings of the first and sheds further light on the underlying mode of thought triggered by these features. In particular, by eliminating a number of potential alternative mechanisms, including online judgments, the mere disruption of decision-related thought, and cognitively constrained conscious deliberation, the second experiment demonstrates that unconscious deliberation is likely to be the underlying cause of superior decision making. With these findings, this research supports a more balanced view in the recent human-computer interaction literature, which suggests that the usual advice to minimize the use of distracting website features should be examined more carefully. The research also uncovers evidence that contributes to the ongoing debate surrounding the D-W-A effect and unconscious thought theory.  
Intraplatform competition has received scant attention in prior studies, which predominantly study interplatform competition. We develop a middle-range theory of how complementarity between input control and a platform extension's modularization—by inducing evolution—influences its performance in a platform market. Primary and archival data spanning five years from 342 Firefox extensions show that such complementarity fosters performance by accelerating an extension's perpetual evolution.  
The information technology (IT) governance literature predominantly explains firms' IT governance choices, but not their strategic consequences. We develop the idea that a firm's IT governance choices induce adeptness at strategically exploiting IT only when they are discriminatingly aligned with its departments' knowledge outside their specialty. Discriminating means that governing the two undertheorized classes of IT assets—apps and infrastructure—requires "peripheral" knowledge in different departments. Analyses of data from 105 firms support our middle-range theory.  
Developing countries, such as India and China, are the fastest growing economies in the world. The successful implementation of information and communication technologies (ICTs) in these countries is likely to hinge on a set of institutional factors that are shaped by the environmental tension between two competing forces, emergent catalysts, such as new economic policies and reform programs, and traditional challenges, such as infrastructure and traditional value systems. To unearth the temporal dynamics underlying the success and failure of ICT implementations in organizations in developing countries, we conducted a two-year multimethod study of an ICT implementation at a large bank in India. Based on data collected from over 1,000 employees and over 1,000 customers, we found, relative to preimplementation levels for up to two years postimplementation, that we characterized as the shakedown phase (1) operational efficiency did not improve, (2) job satisfaction declined, and (3) customer satisfaction declined. In-depth interviews of approximately 40 members of top management, 160 line employees, and 200 customers indicated that these outcomes could be attributed to the strong influence of a set of institutional factors, such as ICT-induced change, labor economics, Western isomorphism, parallel-manual system, and technology adaptation. The interplay between these institutional factors and the environmental tension posed a formidable challenge for the bank during our study that led to the poor and unintended outcomes.  
This paper investigates how citizens' uncertainty in e-government services can be managed. First, we draw from uncertainty reduction theory, and propose that transparency and trust are two key means of reducing citizens' uncertainty in e-government services. Second, we identify two key sets of relevant drivers of e-government service use: (1) information quality characteristics, i.e., accuracy and completeness; and (2) channel characteristics, i.e., convenience and personalization. We propose that the means of uncertainty reduction, information quality characteristics, and channel characteristics are interrelated factors that jointly influence citizens' intentions to use e-government. We tested our model with 4,430 Hong Kong citizens' reactions to two e-government services: government websites and online appointment booking. Our results show that the information quality and channel characteristics predict citizens' intentions to use e-government. Furthermore, transparency and trust mediate as well as moderate the effects of information quality and channel characteristics on intentions. A follow-up survey found that citizens' intentions predict use and ultimately, citizens' satisfaction.  
The emergence of online paid micro-crowdsourcing platforms, such as Amazon Mechanical Turk, allows on-demand and at-scale distribution of tasks to human workers around the world. In such settings, online workers come and complete small tasks posted by employers, working for as long or as little as they wish, a process that eliminates the overhead of hiring (and dismissal). This flexibility introduces a different set of inefficiencies: verifying the quality of every submitted piece of work is an expensive operation that often requires the same level of effort as performing the task itself. A number of research challenges arise in such settings. How can we ensure that the submitted work is accurate? What allocation strategies can be employed to make the best use of the available labor force? How can we appropriately assess the performance of individual workers? In this paper, we consider labeling tasks and develop a comprehensive scheme for managing the quality of crowd labeling: First, we present several algorithms for inferring the true classes of objects and the quality of participating workers, assuming the labels are collected all at once before the inference. Next, we allow employers to adaptively decide which object to assign to the next arriving worker and propose several heuristic-based dynamic label allocation strategies to achieve the desired data quality with significantly fewer labels. Experimental results on both simulated and real data confirm the superior performance of the proposed allocation strategies over other existing policies. Finally, we introduce two novel metrics that can be used to objectively rank the performance of crowdsourced workers after fixing correctable worker errors and taking into account the costs of different classification errors. In particular, the worker value metric directly measures the monetary value contributed by each label of a worker toward meeting the quality requirements and provides a basis for the design of fair and efficient compensation schemes. The online appendix is available at .  
This study investigates users' coping responses in the process of phishing email detection. Three common responses are identified based on the coping literature: task-focused coping, emotion-focused coping (i.e., worry and self-criticism), and avoidance coping. The three responses are used to conceptualize a higher-order construct, coping adaptiveness, that resides on a continuum between maladaptive coping and adaptive coping (manifested as increased task-focused coping and decreased emotion-focused coping and avoidance coping). Drawing on the extended parallel process model and behavioral decision-making literature, this paper examines the antecedents (i.e., perceived phishing threat, perceived detection efficacy, and phishing anxiety) and behavioral consequences (i.e., detection effort and detection accuracy) of coping adaptiveness. A survey experiment with 547 U.S. consumers was conducted. The results show that perceived detection efficacy increases coping adaptiveness. Partially mediated by phishing anxiety, perceived phishing threat decreases coping adaptiveness. Coping adaptiveness positively impacts the two objective measures in the study, detection effort and detection accuracy. The results also suggest that coping adaptiveness and detection effort have different effects on false positives compared to false negatives: detection effort fully mediates the effect of coping adaptiveness on false positive rate (or detection accuracy related to legitimate emails), but has no impact on false negatives (or detection accuracy related to phishing emails), unlike coping adaptiveness. A post hoc analysis on coping responses reveals two patterns of coping among subjects, throwing more light on coping in phishing detection. Theoretical and practical implications are discussed. The online appendix is available at .  
We study the strategic benefits of mergers and acquisitions (M&As) when competing information technology vendors sell different generations of the same product with different quality. We assume the new product arrives unexpectedly when an installed base of the old product exists. We show that the combination of consumers' purchase history and heterogeneity leads to new demand complexity that gives rise to innovative product strategies. We find that shelving the old product is an important motivation for M&A. The acquirer may exercise static or intertemporal price discrimination depending on whether it can exercise upgrade pricing. M&A may speed up or slow down new product consumption, and it can lead to delayed new product introduction in some markets. However, it always increases the acquirer's profit and can sometimes help maximize social welfare. We discuss relevant managerial and policy implications. The online appendix is available at .  
Many e-commerce websites struggle to turn visitors into real buyers. Understanding online users' real-time intent and dynamic shopping cart choices may have important implications in this realm. This study presents an individual-level, dynamic model with concurrent optimal page adaptation that learns users' real-time, unobserved intent from their online cart choices, then immediately performs optimal Web page adaptation to enhance the conversion of users into buyers. To suggest optimal strategies for concurrent page adaptation, the model analyzes each individual user's browsing behavior, tests the effectiveness of different marketing and Web stimuli, as well as comparison shopping activities at other sites, and performs optimal Web page transformation. Data from an online retailer and a laboratory experiment reveal that concurrent learning of the user's unobserved purchase intent and real-time, intent-based optimal interventions greatly reduce shopping cart abandonment and increase purchase conversions. If the concurrent, intent-based optimal page transformation for the focal site starts after the first page view, shopping cart abandonment declines by 32.4% and purchase conversion improves by 6.9%. The optimal timing for the site to intervene is after three page views, to achieve efficient learning of users' intent and early intervention simultaneously.  
This study examines how managerial incentives may drive firms to adopt a proactive strategic posture to implement more information technology (IT) than competitors. We consider both performance incentives that motivate managers to enhance firm returns and risk incentives that motivate managers to take risks. Our empirical analysis shows that while the proactiveness in IT strategic posture leads to both firm returns and firm risk, risk incentives rather than performance incentives essentially drive the proactiveness in IT strategic posture. These findings highlight the issue of managerial risk aversion and the important role of risk incentives in strategic IT decisions. Our study also shows that in diversified firms, risk incentives have a stronger marginal impact on the proactiveness in IT strategic posture in secondary business areas than in primary business areas. Performance incentives, however, may even generate a negative marginal impact on the proactiveness in IT strategic posture in secondary business areas. These results generate important implications for corporate owners regarding how to use various managerial incentives to motivate strategic IT decisions. The online appendix is available at .  
Consumer review systems have become an important marketing communication tool through which consumers share and learn product information. Although there is abundant evidence that consumer reviews have a significant impact on product sales, the design of consumer review systems and its impact on review outcomes and product sales have not yet been well examined. This paper analyzes firms' review system design and product pricing strategies. We formally model two review system design decisions—what rating scale cardinality to use and whether to offer granular review reports. We show that firms' optimal design and pricing strategies critically depend on contextual characteristics such as product valuation, product mainstream level, and consumer misfit cost. Our results suggest that it is beneficial to host a review system only when the product valuation is higher than a threshold. Furthermore, firms should choose low rating scale cardinality for niche products and high rating scale cardinality for mainstream products. When consumers' misfit cost is relatively high, including granular reports in the review system enables firms to attract the favorable consumer segment. Different pricing strategies should be deployed during the initial sale period for different product types. For niche products, firms are advised to adopt lower-bound pricing for high-quality products to take advantage of the positive word of mouth. For mainstream products, firms are advised to adopt upper-bound pricing for high-quality products to enjoy the direct profit from the initial sale period, even after taking into account the negative impact of high price on consumer reviews.  
Health Information Exchanges (HIE) are becoming integral parts of the national healthcare reform efforts, chiefly because of their potential impact on cost reduction and quality enhancement in healthcare services. However, the potential of an HIE platform can only be realized when its multiple constituent users actively participate in using its variety of services. In this research, we model HIE systems as multisided platforms that incorporate self-service technologies whose value to the users depends on both user-specific and networkspecific factors. We develop a model of adoption, use, and involvement of clinical practices in the coproduction of the HIE services. This model is grounded in social network theory, service operations theory, and institutional isomorphism theory. A longitudinal study of actual adoption and use behaviors of 2,054 physicians within 430 community medical practices in Western New York over a three-year period has been carried out to evaluate the proposed model. This study has been supported by HEALTHeLINK, the Regional Health Information Organization of Western New York, which has an extensive database comprising over half a million transactions on patient records by the HIE users. We extracted panel data on adoption, use, and service coproduction behaviors from this database and carried out a detailed analysis using metrics derived from the foundational theories. Positioning practices within two distinct but interrelated networks of patients and practitioners, we show that adoption, use, and service coproduction behaviors are influenced by the topographies of the two networks, isomorphic effects of large practices on the smaller ones, and practice labor inputs in HIE use. Our findings provide a comprehensive view of the drivers of HIE adoption and use at the level of medical practices. These results have implications for marketing and revenue management of HIE platforms, as well as public health and national/regional healthcare policy making.  
Users are increasingly sharing their product interests and experiences with others on e-commerce websites. For example, users can 'tag' products using their own words, and these 'product tags' then serve as navigation cues for other users who want to search for products. Also, socially endorsed information contributors are sometimes highlighted on websites and serve as direct information sources. This study examines the effects of these two distinct social product search cues, product tags and socially endorsed people, on users' perceived diagnosticity and serendipity of their product search experience. While product tags support product navigation via a variety of product features tagged by the community, access to socially endorsed people enables users to browse diverse and high-quality alternatives favored by these individuals. We constructed an experimental website using real data from one of the largest social-network-based product-search websites in China to conduct an empirical study. The results of this study show that product tags help users to locate and evaluate relevant alternatives, thus enhancing the perceived diagnosticity of product search, whereas the integration of product tags and access to socially endorsed people enables users to conduct even more serendipitous searches. In addition, both perceived diagnosticity and perceived serendipity of a search experience positively affect users' decision satisfaction. The online appendix is available at .  
Online labor markets are Web-based platforms that enable buyers to identify and contract for information technology (IT) services with service providers using buyer-determined (BD) auctions. BD auctions in online labor markets either follow an open or a sealed bid format. We compare open and sealed bid auctions in online labor markets to identify which format is superior in terms of obtaining more bids and a higher buyer surplus. Our theoretical analysis suggests that the relative advantage of open versus sealed bid auctions hinges on the role of reducing service providers' valuation uncertainty (difficulty in assessing the cost to execute a project) and competition uncertainty (difficulty in assessing the intensity of the competition from other service providers), which largely depend on the relative importance of the common value (versus the private value) component of the auctioned IT services, calling for an empirical investigation to compare open and sealed bid auctions. Based on a unique data set of 71,437 open bid auctions and 7,499 sealed bid auctions posted by 21,799 buyers at a leading online labor market, we find that, on average, although sealed bid auctions attract 18.4% more bids, open bid auctions offer buyers $10.87 higher surplus. Furthermore, open bid auctions are 55.3% more likely to result in a buyer's selection of a certain service provider and 22.1% more likely to reach a contract (conditional on the buyer's making a selection) with a provider, and they generate higher buyer satisfaction. In contrast to conventional wisdom that "the more bids the better" and industry practice of treating sealed bid auctions as a premium feature, our results suggest that the buyer surplus gained from the reduction in valuation uncertainty enabled by open bid auctions outweighs the buyer surplus gained from the higher competition uncertainty in sealed bid auctions, which renders open bid auctions a superior auction design in online labor markets.  
In the online word-of-mouth literature, research has consistently shown that negative reviews have a greater impact on product sales than positive reviews. Although this negativity effect is well documented at the product level, there is less consensus on whether negative or positive reviews are perceived to be more helpful by consumers. A limited number of studies document a higher perceived helpfulness for negative reviews under certain conditions, but accumulating empirical evidence suggests the opposite. To reconcile these contradictory findings, we propose that consumers can form initial beliefs about a product on the basis of the product's summary rating statistics (such as the average and dispersion of the product's ratings) and that these initial beliefs play a vital role in their subsequent evaluation of individual reviews. Using a unique panel data set collected from Apple's App Store, we empirically demonstrate confirmation bias-that consumers have a tendency to perceive reviews that confirm (versus disconfirm) their initial beliefs as more helpful, and that this tendency is moderated by their confidence in their initial beliefs. Furthermore, we show that confirmation bias can lead to greater perceived helpfulness for positive reviews (positivity effect) when the average product rating is high, and for negative reviews (negativity effect) when the average product rating is low. Thus, the mixed findings in the literature can be a consequence of confirmation bias. This paper is among the first to incorporate the important role of consumers' initial beliefs and confidence in such beliefs (a fundamental dimension of metacognition) into their evaluation of online reviews, and our findings have significant implications for researchers, retailers, and review websites.  
We study the determinants of output quality in offshore business process outsourcing (BPO). Firms can exert control over output quality through incentives formally written into contracts and allow both clients and providers to manage the offshore agents creating a dual governance mechanism. We use a combination of two data sets, a cross sectional data set of 139 processes and a balanced panel data set comprising 21 processes with 36 observations per process, to investigate the impact of different factors on the quality of output of offshore BPO providers. Our findings point to the strong moderating effect of process codifiability on the dual governance mechanism. Process codifiability is not only associated with higher output quality, it also moderates the functioning of the dual governance mechanism and determines when the managerial efforts of the client and provider are substitutes and when they are complementary. We show that contractual incentives tied to quality are generally associated with a higher quality of output. Finally, we show that the use of processlevel inter-organizational information systems also has a positive impact on the output quality of offshore BPO providers.  
When an enterprise system is introduced, system users often experience a performance dip as they struggle with the unfamiliar system. Appropriately managing this phase, which we term as the swift response phase (SRP), is vital given its prominent impact on the eventual success of the system. Yet, there is a glaring lack of studies that examine the SRP. Drawing on sensemaking theory and early postadoptive literature, this study seeks to propose a theory-driven model to understand how different support structures facilitate different forms of use-related activities to induce a positive performance in the SRP. The model was tested through a two-stage survey involving 329 nurses. The results demonstrated the discriminating alignment between information system (IS) use-related activity and support structures in enhancing system users' work performance in the SRP. Specifically, suitability of impersonal support moderated the effects of standardized system use and individual adaption on performance, whereas availability of personal support only moderated the effect of nonstandardized system use on performance. For moderating role of personal support, IS specialists support had a lower influence than peer-champion support and peer-user support. This study contributes to the extant literature by (1) conceptualizing the turbulent SRP, (2) applying sensemaking theory to the initial postadoptive stage, (3) adding to the theoretical debate on the value of system use, and (4) unveiling the distinct roles of support structures under different types of use activities. Practical suggestions are provided for organizational management and policy makers to deal with the complexities in the SRP.  
Information systems (IS) research usually investigates phenomena at one level of analysis at a time. However, complex IS phenomena may be difficult to address from such a single-level perspective. A multilevel perspective offers an alternative means to examine phenomena by simultaneously accounting for multiple levels of analysis. Although useful guidelines for theory development are widely available, they give little specific attention to developing theory that is conceptualized and analyzed at multiple levels. Multilevel theorizing or developing theory from a multilevel perspective is more complex and involves unique challenges. To promote multilevel theorizing in the IS discipline, we focus on addressing challenges involved in multilevel theorizing and propose a holistic framework for systematically developing theory from a multilevel perspective. Drawing from the organization science and IS literature, the proposed framework harmonizes and synthesizes previous guidelines, providing a practical basis for conceptualizing and studying multilevel phenomena. The online appendix is available at .  
The sale of genuinely branded products through unauthorized channels (also known as gray markets) is a growing problem for many firms that operate in separate markets. It is generally believed that the existence of such unauthorized sales will cannibalize the profits of brand owners. In this paper, we develop a pricing model for a firm that sells an identical product in two distinct markets but faces the threat of potential gray market sales. The firm chooses prices in each market. A consumer chooses whether to buy the product from one of the markets including a gray market. We derive the optimal prices in the two markets and examine their effects on consumer demand and the total profit. We show that the higher price in one market transfers part of its demand into the gray market, thus influencing the consumer demand in the low-priced market as well. Additionally, the price gap between the two separate markets positively influences gray market sales and, under certain conditions, can lead to an increase in firm profit. Using authorized sales data from a Fortune 100 company and a separate data set on online gray market sales, we find empirical evidence in support of our model results.  
Can location-based mobile promotion (LMP) trigger contemporaneous and delayed sales purchases? As mobile technologies can reach users anywhere and anytime, LMP becomes a promising new channel. We unravel the dynamic sales impact of LMP on the basis of a randomized field experiment with 22,000 mobile users sponsored by one of the largest mobile service providers in the world. Our identification strategy is to gauge the marginal increases in consumer purchases of the geo-fenced treatment group of users who received LMP, above and beyond the baseline control groups. There are two controls: one group who received the same LMP but not in the virtual geo-fencing locational range (nongeo-fenced control), and the other who did not receive the LMP but in the geo-fencing range (geo-fenced control). The latter control serves as an organic holdout baseline from the similar population, i.e., counterfactual test of what if without the mobile LMP intervention, to identify the actual "lift" of incremental purchases caused by the treatment with the mobile LMP intervention. Findings suggest that LMP treatment has a significantly stronger impact on contemporaneous (same-day) purchases and delayed (subsequent-days) purchases than the controls. The randomized experiment design renders these findings robust to alternative explanations such as mobile usage behavior heterogeneity, product effects heterogeneity, nonrandomized sample-selection bias, and endogeneity concerns. Follow-up surveys with the field experiment users explore the nuanced mechanisms via which LMP may induce the impulsive, same-day purchases, and create product awareness for the planned subsequent-days purchases. LMP can generate six times more purchases than nongeo-fenced control with the LMP intervention, and 12 times more than geo-fenced control without the LMP intervention. LMP has a delayed sales effect for 12 days after the mobile promotions. The total sales impact of LMP could be underestimated by 54% if excluding the delayed sales impact and only including the contemporaneous impact. These findings are new to the literature and often neglected in mobile promotion practices, proffering novel implications on the sales value of LMP in the mobile era.  
Advances in the digital economy have driven the trend among manufacturers, particularly those in the information technology (IT) industry, to offer products that consumers can self-customize to satisfy their idiosyncratic needs. This study examines firm strategies on offering such consumer-customizable products. Our analysis shows that a monopolistic firm obtains a greater profit from offering a consumer-customizable product than from offering a preconfigured standardized product only if consumers are sufficiently capable to conduct the customization task; otherwise, it is more profitable for the firm to offer a standardized product. Moreover, consumers obtain a greater surplus when the firm offers the customizable product. We also consider the case where the firm is capable of offering both a customizable product and a standardized product and find that the firm benefits more from offering both products than offering either product if consumer customizing capability and the customization cost are not too high. Interestingly, when the firm offers both products, its effort in enhancing consumer customizability (e.g., offering free consumer training) always benefits both the firm and consumers, but its effort in increasing the value of the standardized product (e.g., offering more functions) can hurt both the firm profit and consumer surplus. Our theoretical results explain many interesting business practices and provide useful insights for marketing practitioners.  

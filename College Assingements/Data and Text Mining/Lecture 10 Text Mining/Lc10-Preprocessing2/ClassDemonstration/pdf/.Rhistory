ruz = apriori(custsub , parameter = list(support = 0.3 , conf = 0.5 ))
inspect(ruz)
id<- c(1,2,3,4,5,6)
gender<- c("female","male","female","female","male","female")
age <- c(23,28,42,34,45,36)
child <-c("no","no","no","yes","no","yes")
mobile_app <- c("yes","yes","no","yes","no","yes")
reorder <- c("yes","no","no","yes","no","yes")
cust = cbind(id,gender,age,child,mobile_app,reorder)
# Creating the Data frame with Cust
cust = transform(cust,
id = as.character(id),
#                 gender = as.character(gender),
age = as.character(age),
#                child = as.character(child),
#                 mobile_app = as.character(mobile_app),
#                reorder=as.character(reorder)
)
# Discreation on age data
cust = within (cust,{
age_ds =character(0)
age_ds[age<=29] ="age_20"
age_ds[age > 29 && age <= 30] = "age_30"
age_ds[age > 30] = "age_40"
#   age_ds = as.factor(age_ds, level=c("age_20","age_30","age_40"))
})
cust$age_ds <- as.factor(cust$age_ds)
custsub <- subset(cust , select = -c(id,age))
library(arules)
ruz = apriori(custsub , parameter = list(support = 0.3 , conf = 0.5 ))
inspect(ruz)
id<- c(1,2,3,4,5,6)
gender<- c("female","male","female","female","male","female")
age <- c(23,28,42,34,45,36)
child <-c("no","no","no","yes","no","yes")
mobile_app <- c("yes","yes","no","yes","no","yes")
reorder <- c("yes","no","no","yes","no","yes")
cust = cbind(id,gender,age,child,mobile_app,reorder)
# Creating the Data frame with Cust
cust = transform(cust,
id = as.character(id),
#                 gender = as.character(gender),
age = as.character(age)
#                child = as.character(child),
#                 mobile_app = as.character(mobile_app),
#                reorder=as.character(reorder)
)
# Discreation on age data
cust = within (cust,{
age_ds =character(0)
age_ds[age<=29] ="age_20"
age_ds[age > 29 && age <= 30] = "age_30"
age_ds[age > 30] = "age_40"
#   age_ds = as.factor(age_ds, level=c("age_20","age_30","age_40"))
})
cust$age_ds <- as.factor(cust$age_ds)
custsub <- subset(cust , select = -c(id,age))
library(arules)
ruz = apriori(custsub , parameter = list(support = 0.3 , conf = 0.5 ))
inspect(ruz)
install.packages("tm")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("SnowballC")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("wordcloud")
install.packages("lsa")
install.packages("pdftools")
install.packages("qdapTools")
install.packages("stringr")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
install.packages("tm")
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
`1` <- read.csv("~/GitHub/Texas-Tech-University/College Assingements/Data and Text Mining/Lecture 10 Text Mining/Lc10-Preprocessing2/ClassDemonstration/txt/1.txt", sep="")
View(`1`)
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\Users\dubey\Documents\GitHub\Texas-Tech-University\College Assingements\Data and Text Mining\Lecture 10 Text Mining\Lc10-Preprocessing2\ClassDemonstration\txt"
setwd(file_path)
dir(file_path)
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
Files <- list.files(pattern = "pdf$")
Files
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
Files <- list.files(pattern = "pdf$")
text <- lapply(Files, pdf_text)
text
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
Files <- list.files(pattern = "pdf$")
#converting pdf to text
text <- lapply(Files, pdf_text)
# Removing extra char
text_clean <- lapply(text, str_replace_all,"[\n], ")
text_clean
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
Files <- list.files(pattern = "pdf$")
#converting pdf to text
text <- lapply(Files, pdf_text)
# Removing extra char
text_clean <- lapply(text, str_replace_all,"[\n]","")
text_clean
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
text_clean <- lapply(text, str_replace_all,"[\n]","")
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
Files <- list.files(pattern = "pdf$")
#converting pdf to text
text <- lapply(Files, pdf_text)
# Removing extra char
text_clean <- lapply(text, str_replace_all,"[\n]","")
text_clean
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
library(stringr)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
Files <- list.files(pattern = "pdf$")
#converting pdf to text
text <- lapply(Files, pdf_text)
# Removing extra char
text_clean <- lapply(text, str_replace_all,"[\n]","")
text_clean
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
library(stringr)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
Files <- list.files(pattern = "pdf$")
#converting pdf to text
text <- lapply(Files, pdf_text)
# Removing extra char
text_clean <- lapply(text, str_replace_all,"[\n]","")
text_clean1 <- lapply(text_clean, str_replace_all,"[\r]","")
text_clean1
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
library(tm)
library(pdftools)
library(qdapTools)
library(SnowballC)
library(wordcloud)
library(Matrix)
library(stringr)
file_path <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\txt"
setwd(file_path)
dir(file_path)
corpus_txt <- Corpus(DirSource(file_path))
corpus_txt
file_path1 <- "C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\pdf"
setwd(file_path1)
dir(file_path1)
corpus_txt1 <- Corpus(DirSource(file_path1))
corpus_txt1
Files <- list.files(pattern = "pdf$")
#converting pdf to text
text <- lapply(Files, pdf_text)
# Removing extra char
text_clean <- lapply(text, str_replace_all,"[\n]","")
text_clean1 <- lapply(text_clean, str_replace_all,"[\r]","")
text_clean1
corpus_txt_2 <- Corpus(VectorSource(text_clean1))
corpus_txt_2
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt")
HBO
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
text.decom
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
corpus_txt_3 <- Corpus(VectorSource(text.decom))
corpus_txt_3
stopwords("en")
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
corpus_txt_3 <- Corpus(VectorSource(text.decom))
corpus_HBO <- tm_map(corpus_txt_3, PlainTextDocument )
corpus_HBO <- tm_map(corpus_txt_3, tolower )
corpus_HBO <- tm_map(corpus_txt_3, removeNumbers )
corpus_HBO <- tm_map(corpus_txt_3, removePunctuation )
stopwords(kind = "en")
mystopwords <- c("HBO" , "hbo" ,"now" ,"app")
corpus_HBO_stop <- tm_map(corpus_HBO , removeWords , c(stopwords(kind = "en") , mystopwords))
corpus_HBO_stop
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
corpus_txt_3 <- Corpus(VectorSource(text.decom))
corpus_HBO <- tm_map(corpus_txt_3, PlainTextDocument )
corpus_HBO <- tm_map(corpus_txt_3, tolower )
corpus_HBO <- tm_map(corpus_txt_3, removeNumbers )
corpus_HBO <- tm_map(corpus_txt_3, removePunctuation )
stopwords(kind = "en")
mystopwords <- c("HBO" , "hbo" ,"now" ,"app")
corpus_HBO_stop <- tm_map(corpus_HBO , removeWords , c(stopwords(kind = "en") , mystopwords))
corpus.tbm <- TermDocumentMatrix(corpus_HBO_stop)
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
corpus_txt_3 <- Corpus(VectorSource(text.decom))
corpus_HBO <- tm_map(corpus_txt_3, PlainTextDocument )
corpus_HBO <- tm_map(corpus_txt_3, tolower )
corpus_HBO <- tm_map(corpus_txt_3, removeNumbers )
corpus_HBO <- tm_map(corpus_txt_3, removePunctuation )
stopwords(kind = "en")
mystopwords <- c("HBO" , "hbo" ,"now" ,"app")
corpus_HBO_stop <- tm_map(corpus_HBO , removeWords , c(stopwords(kind = "en") , mystopwords))
corpus.tbm <- TermDocumentMatrix(corpus_HBO_stop)
write.csv(as.matrix(corpus.tbm) , file=file.path("tdm.csv"))
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
corpus_txt_3 <- Corpus(VectorSource(text.decom))
corpus_HBO <- tm_map(corpus_txt_3, PlainTextDocument )
corpus_HBO <- tm_map(corpus_txt_3, tolower )
corpus_HBO <- tm_map(corpus_txt_3, removeNumbers )
corpus_HBO <- tm_map(corpus_txt_3, removePunctuation )
stopwords(kind = "en")
mystopwords <- c("HBO" , "hbo" ,"now" ,"app")
corpus_HBO_stop <- tm_map(corpus_HBO , removeWords , c(stopwords(kind = "en") , mystopwords))
corpus.tbm <- TermDocumentMatrix(corpus_HBO_stop)
write.csv(as.matrix(corpus.tbm) , file=file.path("tdm.csv"))
freq <- rowSums(as.matrix(corpus.tbm))
freq <- sort(freq , decreasing = TRUE)
head(freq)
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
corpus_txt_3 <- Corpus(VectorSource(text.decom))
corpus_HBO <- tm_map(corpus_txt_3, PlainTextDocument )
corpus_HBO <- tm_map(corpus_txt_3, tolower )
corpus_HBO <- tm_map(corpus_txt_3, removeNumbers )
corpus_HBO <- tm_map(corpus_txt_3, removePunctuation )
stopwords(kind = "en")
mystopwords <- c("HBO" , "hbo" ,"now" ,"app")
corpus_HBO_stop <- tm_map(corpus_HBO , removeWords , c(stopwords(kind = "en") , mystopwords))
corpus.tbm <- TermDocumentMatrix(corpus_HBO_stop)
write.csv(as.matrix(corpus.tbm) , file=file.path("tdm.csv"))
freq <- rowSums(as.matrix(corpus.tbm))
freq <- sort(freq , decreasing = TRUE)
head(freq)
words <- names(freq)
wordcloud(words[1,40] , freq[1:40] , scale = c(2,0.8) , colors = brewer.pal(8,"Dark2"))
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
corpus_txt_3 <- Corpus(VectorSource(text.decom))
corpus_HBO <- tm_map(corpus_txt_3, PlainTextDocument )
corpus_HBO <- tm_map(corpus_txt_3, tolower )
corpus_HBO <- tm_map(corpus_txt_3, removeNumbers )
corpus_HBO <- tm_map(corpus_txt_3, removePunctuation )
stopwords(kind = "en")
mystopwords <- c("HBO" , "hbo" ,"now" ,"app")
corpus_HBO_stop <- tm_map(corpus_HBO , removeWords , c(stopwords(kind = "en") , mystopwords))
corpus.tbm <- TermDocumentMatrix(corpus_HBO_stop)
write.csv(as.matrix(corpus.tbm) , file=file.path("tdm.csv"))
freq <- rowSums(as.matrix(corpus.tbm))
freq <- sort(freq , decreasing = TRUE)
head(freq)
words <- names(freq)
wordcloud(words[1: 40] , freq[1:40] , scale = c(2,0.8) , colors = brewer.pal(8,"Dark2"))
HBO <- file("C:\\Users\\dubey\\Documents\\GitHub\\Texas-Tech-University\\College Assingements\\Data and Text Mining\\Lecture 10 Text Mining\\Lc10-Preprocessing2\\ClassDemonstration\\HBO_NOW.txt" , open = "r")
text.decom = readLines(HBO)
text.decom[1]
corpus_txt_3 <- Corpus(VectorSource(text.decom))
corpus_HBO <- tm_map(corpus_txt_3, PlainTextDocument )
corpus_HBO <- tm_map(corpus_txt_3, tolower )
corpus_HBO <- tm_map(corpus_txt_3, removeNumbers )
corpus_HBO <- tm_map(corpus_txt_3, removePunctuation )
stopwords(kind = "en")
mystopwords <- c("HBO" , "hbo" ,"now" ,"app")
corpus_HBO_stop <- tm_map(corpus_HBO , removeWords , c(stopwords(kind = "en") , mystopwords))
corpus.tbm <- TermDocumentMatrix(corpus_HBO_stop)
write.csv(as.matrix(corpus.tbm) , file=file.path("tdm.csv"))
freq <- rowSums(as.matrix(corpus.tbm))
freq <- sort(freq , decreasing = TRUE)
head(freq)
words <- names(freq)
wordcloud(words[1: 40] , freq[1:40] , scale = c(2,0.8) , colors = brewer.pal(8,"Dark2"))
Train_UWu5bXk <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
View(Train_UWu5bXk)
Test_u94Q5KV <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
View(Test_u94Q5KV)
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
str(Train)
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
str(Train)
summary(Train)
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
str(Train)
summary(Train)
is.null(Train)
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
str(Train)
summary(Train)
is.null(Train$Item_Weight)
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
str(Train)
summary(Train)
is.na(Train)
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
str(Train)
summary(Train)
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
str(Train)
summary(Train)
# Analysing the Data
boxplot(Item_Outlet_Sales~*, data=Train, notch=TRUE,
col=(c("gold","darkgreen")),
main="Item Sales", xlab="Outlet")
boxplot(Item_Outlet_Sales~, data=Train, notch=TRUE,col=(c("gold","darkgreen")),main="Item Sales", xlab="Outlet")
boxplot( data=Train, notch=TRUE,col=(c("gold","darkgreen")),main="Item Sales", xlab="Outlet")
# Importing the Data set
Train <- read.csv("D:/Kaggle Projects/Sale Prediction/Train_UWu5bXk.csv")
Test <- read.csv("D:/Kaggle Projects/Sale Prediction/Test_u94Q5KV.csv")
# Exploring the Data
head(Train)
str(Train)
summary(Train)
# Analysing the Data
boxplot( Item_Outlet_Sales~Item_Identifier+ Item_Weight,data=Train, notch=TRUE,col=(c("gold","darkgreen")),main="Item Sales", xlab="Outlet")
